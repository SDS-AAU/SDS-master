---
title: 'Machine Learning: Workflow and Applications'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 1
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```

```{r}
### Load standardpackages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)
```

# Introduction: Machine Learning with Pokémon

Back to the teens. You will work on Pokemon data. No data munging needed.

# Getting the data

```{r}
data <- read_csv('https://sds-aau.github.io/SDS-master/00_data/pokemon.csv')
```

# EDA & Unsupervised ML

## 1.Give a brief overview of data, what variables are there, how are the variables scaled and variation of the data columns.

```{r}
data %>% head()
```

```{r}
data %>% glimpse()
```

```{r}
library(skimr)
data %>% skim()
```

We have 3 string variables, where one is the uniqu pokemon name, and two others it's type. Our outcome variable `legendary` is categorical, `TRUE` if it is an legendary pokemon, `FALSE` otherwise. All other variables are numerical. 

Lets briefly look at th categorical variables:

```{r}
data %>% count(Type1, sort = TRUE)
```

We see most pokemon are water-pokemon. However, there is no super-sparse class, so we can work with that.

```{r}
data %>% count(Type2, sort = TRUE)
```

We see most pokemons have an `NA` as `Type2`, probably meening they have no second type.

We directly see some things should be changed:

1. We see an numeric `Number` variable, which is just their unique id, and not of value for the analysis, so we drop it.
2. the missing values in `Type2` probably indicate that the pokemon has no second type, should therefore not be treated as missing data
3. The `generation` variable is numeric, but probably should be interpreted as categorical.

Lets do the necessary changes upfront before investigating further.

```{r}
data %<>%
  select(-Number) %>%
  replace_na(list(Type2 = 'None')) %>%
  mutate(Generation = Generation %>% as.character()) %>%
  relocate(Name, Legendary)
```

We now can do some visualization. First my favorite standard one. Since we already know we later will be interested in legendary pokemon, we can already use the olor aestetic on this variable.

```{r, fig.height=12, fig.width=12}
library(GGally)
data %>% 
  select(-Name, -Type1, -Type2) %>%
  ggpairs(legend = 1,
          mapping = ggplot2::aes(colour=Legendary, alpha = 0.5), 
          lower = list(continuous = wrap("smooth", alpha = 0.3, size=0.1))) +
  theme(legend.position = "bottom")  
```
Long story short, we already see that legendary pokemon appear to be pretty much better in everything.

We can also zoom into the conditional distributions with a joyplot

```{r,fig.height=5,fig.width=12.5}
library(ggridges) # install if necessary

data %>%
  gather(variable, value, -Legendary) %>% # Note: At one point do pivot_longer instead
  ggplot(aes(y = as.factor(variable), 
             fill =  as.factor(Legendary), 
             x = percent_rank(value)) ) +
  ggridges::geom_density_ridges(alpha = 0.75)
```

## 2. Execute a PCA analysis on all numerical variables in the dataset. Hint: Don't forget to scale them first. Use 4 components. What is the individuel and cumulative explained variance? 

```{r,warning=FALSE,echo=FALSE}
# Load packages
library(FactoMineR)
library(factoextra)
```

```{r}
res_pca <- data %>% 
  column_to_rownames('Name') %>%
  select_if(is_numeric) %>%
  PCA(scale.unit = TRUE, 
      graph = FALSE, 
      ncp = 4)
```


```{r,fig.align='center'}
res_pca %>% 
  fviz_screeplot(addlabels = TRUE, 
                 ncp = 4)
```
```{r}
res_pca$eig %>% as_tibble()
```

4 Components sem to capture about 90% of the variance.

BONUS: Some visualization:

```{r,fig.width=10,fig.height=10,fig.align='center'}
res_pca %>%
  fviz_pca_var(alpha.var = "cos2",
               col.var = "contrib",
               gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
               repel = TRUE,
               ggtheme = theme_gray()) 
```

All variables pretty much point into the same direction, meaning that we mainly capture a scaling phenomenon, where variables tend to move jointly. However, we do see in the secomd component, variables seem to split between defensive and offensive attributes.

```{r,,fig.width=10,fig.height=10,fig.align='center'}
res_pca %>% 
  fviz_pca_biplot(alpha.ind = "cos2",
                  geom = "point",                   
                  habillage = data %>% pull(Legendary) %>% factor(), 
                  addEllipses = TRUE,
                  ggtheme = theme_gray()) 
```

We generally see again legendary pokemon to be more in the right quadrants indicating high values in their characteristics. However, there is a large grey-zone, where the 2d projection cannot really distinguist between legendary and non-legendary pokemons.

```{r,,fig.width=10,fig.height=10,fig.align='center'}
res_pca %>% 
  fviz_pca_biplot(alpha.ind = "cos2",
                  geom = "point",                   
                  habillage = data %>% pull(Type1) %>% factor(), 
                  addEllipses = TRUE,
                  ggtheme = theme_gray()) 
```

For `Type1`, we get a mess...

## Use a different dimensionality reduction method (eg. UMAP/NMF) –do the findings differ?

We ill run a simple UMAP

```{r}
library(uwot)
res_umap <-data%>%
  select_if(is_numeric) %>%
  umap(n_neighbors = 15, 
       metric = "cosine", 
       min_dist = 0.01, 
       scale = TRUE) 
```

```{r}
res_umap %>%
  as_tibble() %>%
  bind_cols(data %>% select(Legendary)) %>%
  ggplot(aes(x = V1, y = V2, col = Legendary)) + 
  geom_point(shape = 21, alpha = 0.5) 
```

Well, UMAP seems to do a better job in sepperation high and low performing pokemons in spacw. However, we also see that only somewhat helps us to distinguish between legendary and non-legendary ones.

```{r}
res_umap %>%
  as_tibble() %>%
  bind_cols(data %>% select(Type1)) %>%
  ggplot(aes(x = V1, y = V2, col = Type1)) + 
  geom_point(shape = 21, alpha = 0.5) 
```
Same goes for types...

## 4. Perform a cluster analysis (KMeans) on all numerical variables (scaled & before PCA). Pick a realistic number of clusters (up to you where the large clusters remain mostly stable).

```{r,fig.align='center'}
# We use the viz_nbclust() function of the factorextra package
data %>%
  select_if(is_numeric) %>% 
  scale() %>%
  fviz_nbclust(kmeans, method = "wss")  
```
Ok,we here settle for 2 (executive decision, since we want to identify 2 distinct classes). 

```{r}
res_km <- data %>% 
  column_to_rownames('Name') %>%
  select_if(is_numeric) %>%
  scale() %>% 
  kmeans(centers = 2, nstart = 20)  
```

## 5.Visualize the first 2 principal components and color the datapoints by cluster.

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_km %>% 
  fviz_cluster(data = data %>% select_if(is_numeric))  
```

## 6.Inspect the distribution of the variable “Type1” across clusters. Does the algorithm separate the different types of pokemon?

```{r}
table(data$Type1, res_km$cluster)
```

Well, not really. However, in case I would have selected more, for instance 4, clusters, I might have had a different picture.

```{r}
data %>%
  bind_cols(cluster = res_km$cluster) %>%
  select_if(is_numeric) %>%
  group_by(cluster) %>%
  mutate(n = n()) %>%
  summarise_all(funs(mean)) %>%
  pivot_longer(-cluster) %>%
  pivot_wider(names_from = cluster, values_from = value)
```

As we already guessed, the 2 clusters are mainly formed by overall high or low attributes.

## 7.Perform a cluster analysis on all numerical variables scaled and AFTER dimensionality reduction and visualize the first 2 principal components.

Since we didnt specify which type of clustering, we cann also do hirarchical clustering now and use the HCPCA function for convenience.

```{r}
res_hcpc <- res_pca %>% 
  HCPC(nb.clust = -1, #  self determined: higher relative loss of inertia
       graph = FALSE) 
```

```{r,,fig.width=15,fig.height=10,fig.align='center'}
res_hcpc %>%
  plot(choice = "3D.map")
```

Interestingly, here we would have 3 clusters, with one in the middle.

## 8.Again, inspect the distribution of the variable “Type 1” across clusters, does it differ from the distribution before dimensionality reduction?

```{r}
table(data$Type1, res_hcpc$data.clust$clust)
```

Well, not reall in most cases...

# Supervised ML

Your task will be to predict the variable “legendary”, indicating if the pokemon is alegendary one or not.

## 1.Perform necessary ML preprocessing of your data if deemed necessary.
## 2.Split the data in a training (75%) and test (25%) dataset

```{r}
library(tidymodels)
```

One more thing, we have a logical outcome variable. That causes problems with some models which require factorial outcomes, so we recode it to a factor. And for sure we also drop the `Name` column which we do not want to use for the prediction.

```{r}
data %<>%
  rename(y = Legendary) %>%
  mutate(y = y %>% factor()) %>%
  select(y, everything(), -Name)
```



First we split...

```{r}
data_split <- initial_split(data, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```

Now we will define a pretty standard preprocessing reciple, where we will:

1. Center and scale all numeric variables
2. Create one-hot-encodings for all categorical variables

```{r}
data_recipe <- data_train %>%
  recipe(y ~.) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes(), one_hot = TRUE) 
```

## 3.Define a n-fold cross-validation workflow for your model testing.

From 4. we already know which models to set up, so we already set up the models. This is pretty much all only C&P from my script...


### Defining the models

#### Logistic Regression

```{r}
model_lg <- logistic_reg(mode = 'classification') %>%
  set_engine('glm', family = binomial) 
```

#### Decision tree

```{r}
model_dt <- decision_tree(mode = 'classification',
                          cost_complexity = tune(),
                          tree_depth = tune(), 
                          min_n = tune()
                          ) %>%
  set_engine('rpart') 
```

#### Extreme Gradient Boosted Tree (XGBoost)

```{r}
model_xg <- boost_tree(mode = 'classification', 
                       trees = 100,
                       mtry = tune(), 
                       min_n = tune(), 
                       tree_depth = tune(), 
                       learn_rate = tune()
                       ) %>%
  set_engine("xgboost") 
```

#### Define workflow

```{r}
workflow_general <- workflow() %>%
  add_recipe(data_recipe) 

workflow_lg <- workflow_general %>%
  add_model(model_lg)

workflow_dt <- workflow_general %>%
  add_model(model_dt)

workflow_xg <- workflow_general %>%
  add_model(model_xg)
```

### Hyperparameter Tuning

#### Validation Sampling (N-fold crossvlidation)

```{r}
data_resample <- data_train %>% 
  vfold_cv(strata = y,
           v = 3,
           repeats = 3)
```

#### Hyperparameter Tuning: Decision Tree

```{r}
tune_dt <-
  tune_grid(
    workflow_dt,
    resamples = data_resample,
    grid = 10
  )
```

```{r}
tune_dt %>% autoplot()
```

```{r}
best_param_dt <- tune_dt %>% select_best(metric = 'roc_auc')
best_param_dt
```

```{r}
tune_dt %>% show_best(metric = 'roc_auc', n = 1)
```

#### Hyperparameter Tuning: XGBoost

```{r}
tune_xg <-
  tune_grid(
    workflow_xg,
    resamples = data_resample,
    grid = 10
  )
```

```{r}
tune_xg %>% autoplot()
```

```{r}
best_param_xg <- tune_xg %>% select_best(metric = 'roc_auc')
best_param_xg
```

```{r}
tune_xg %>% show_best(metric = 'roc_auc', n = 1)
```

## 4.Fit three separate models on your training data, where you predict the“legendary” variable. Use a 1. Logistic regression, 2. Decision tree, and 3.(minimum)on adittional SML algorithm of choice to do so.

### Finalize workflow

```{r}
workflow_final_dt <- workflow_dt %>%
  finalize_workflow(parameters = best_param_dt)

workflow_final_xg <- workflow_xg %>%
  finalize_workflow(parameters = best_param_xg)
```

### Fit Model on training data

```{r}
fit_lg <- workflow_lg %>%
  fit(data_train)

fit_dt <- workflow_final_dt %>%
  fit(data_train)

fit_xg <- workflow_final_xg %>%
  fit(data_train)
```

### Compare performance (On training data)

```{r}
pred_collected <- tibble(
  truth = data_train %>% pull(y) %>% as.factor(),
  #base = mean(truth),
  lg = fit_lg %>% predict(new_data = data_train) %>% pull(.pred_class),
  dt = fit_dt %>% predict(new_data = data_train) %>% pull(.pred_class),
  xg = fit_xg %>% predict(new_data = data_train) %>% pull(.pred_class),
  ) %>% 
  pivot_longer(cols = -truth,
               names_to = 'model',
               values_to = '.pred')
```

```{r}
pred_collected %>%
  group_by(model) %>%
  bal_accuracy(truth = truth, estimate = .pred) %>%
  select(model, .estimate) %>%
  arrange(desc(.estimate))
```
## 5.Use the fitted models to predict the “legendary” variable in your test data.


That's easy. We will jut use the `last_fit` function which fits the final model on the complete training data and predicts on the test data in one call.

```{r}
fit_last_lg <- workflow_lg %>% last_fit(split = data_split)
fit_last_dt <- workflow_dt %>% last_fit(split = data_split)
fit_last_xg <- workflow_xg %>% last_fit(split = data_split)
```


## 6.Evaluate the performance of these 3 models by comparing the predicted and thetrue values of “legendary” in the test data. To do so, also create a confusionmatrix,provide and discuss further useful metrics of model performance.

```{r}
fit_last_lg %>% collect_metrics()
```
```{r}
fit_last_dt %>% collect_metrics()
```

```{r}
fit_last_xg %>% collect_metrics()
```