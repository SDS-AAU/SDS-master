---
title: 'Neural Networks Application: Ecosystem & simple ANNs (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(magrittr)
```


# Introduction to `Keras`

* There are quite a bunch of deep learning frameworks around, from the older `Caffee` and `Theano` to Google's `Tensorflow` and the newer `Pytorch` (which is increasingly trending in research). 
* However, during the rest of this course, 95% of our deep learning exercises will be done using `Keras`
* Keras is a deep-learning framework that provides a convenient way to define and train almost any kind of deep-learning model. Keras was initially developed for researchers, with the aim of enabling fast experimentation. 

It has the following advantages:

* User-friendly API which makes it easy to quickly prototype deep learning models.
* Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.
* Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc., is therefore appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.
* Is capable of running on top of multiple back-ends including `TensorFlow`, `CNTK`, or `Theano`.
* Allows the same code to run on CPU or on GPU, and has strong multi-GPU, distributed storage, and training support (`Google cloud`, `Spark`, `HDF5`...)
* Can easily be integrated in AI products (Apple `CoreML`, `TensorFlow` Android runtime,  `R` or `Python` webapp backend such as a `Shiny` or `Flask` app)

It is widely adapted in academia and industry (Google, Netflix, Uber, CERN, Yelp, Square etc.), and is also a popular framework on Kaggle, the machine-learning competition website, where almost every recent deep-learning competition has been won using `Keras` models. While Google's `TensorFlow` is even more popular, keep in mind that `Keras` can use `Tensorflow` (and other popular DL frameworks) as backend, and allows less cumbersome and more high-level 

<img width="49%" src="https://sds-aau.github.io/SDS-master/00_media/dl_frameworks_1.jpg"/>
<img width="49%" src="https://sds-aau.github.io/SDS-master/00_media/dl_frameworks_2.png"/>

* So, after all, `Keras` represents a wonderful high-level starter, fast and easy implementable, and in most cases flexible enough to do whatever you feel like.

![](https://sds-aau.github.io/SDS-master/00_media/DL_keras_wtf.png){width=750px}

**Sidenote:** The weird name (`Keras`) means *horn* in Greek, and is a reference to ancient Greek literature. Eg., in Odyssey, supernatural *dream spirits* are divided between those who deceive men with false visions (arriving to Earth through a gate of ivory), and those who announce a future that will come to pass (arriving through a gate of horn). So, enough history lessons, let's run our first deep learning model!

```{r}
# Load our main tool
library(keras)
```

# Our first deep learning model

## Introduction

* Well, its about time to get serious. We will dive straight in, and use a simple deep learning model on the classical `Mnist` dataset. 
* This is the original data used by Jan LeCun and his team to fit an ANN that identifies handwritten digits for the US postal service. 
* It consists of quite a bunch of samples of handwritten dicites together with their correct label. The wandwritten dicits here conveniently come as a 28x28 greyscale matrix, making them a good starter to warm up. Lets do that.


## Load our data and get ready

```{r}
# Load our data
mnist <- dataset_mnist()
```

```{r}
mnist %>%
  glimpse()
```


```{r}
# sepperate in train and test
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

* Lets take a look at the structure.

```{r}
glimpse(train_images)
```

```{r}
glimpse(train_labels)
```

```{r}
digit <- train_images[5,,]
digit[,8:20] # I crop it a bit, otherwise the columns dont fit on one page
```

To make it more tangible, lets plot one:

```{r}
digit %>% as.raster(max = 255) %>% plot()
```

```{r}
rm(digits)
```


## Define the `Keras` model

The workflow will be as follows: 

1. First, we'll feed the neural network the training data, `train_images` and `train_labels`. 
2. The network will then learn to associate images and labels. 
3. Finally, we'll ask the network to produce predictions for `test_images`, and we'll verify whether these predictions match the labels from `test_labels`.

Let's build the network - again, remember that you aren't expected to understand everything about this example yet.

Building a model in `Keras` that can be fitted on your data involves two steps:

1. Defining the networks architecture interms of layers and their shape.
2. Compiling the model, and defining the loss function, evaluation metric, and optimizer.

```{r}
network <- keras_model_sequential() %>% 
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

Notice that the layer stacking in `R` is done via the well-known `%>%`, in `Pyhton` with `.`. That's about the main difference between both implementations. 

* The core building block of neural networks is the **layer**, a data-processing module that you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. 
* Specifically, layers extract representations out of the data fed into them - hopefully, representations that are more meaningful for the problem at hand. 
* Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. 

* Here, our network consists of a sequence of two layers, which are **densely connected** (`layer_dense`) neural layers. 
* The second (and last) layer is a 10-way `softmax` layer, which means it will return an array of 10 probability scores (summing to 1). 
* Each score will be the probability that the current digit image belongs to one of our 10 digit classes. So, we defined a network with overall 634 cells, consisting of:
   1. input layer: 28x28 = 512 cells
   2. intermediate layer : 28x28  = 512 cells
   3. Output layer: 10 cells

* To make the network ready for training, we need to pick three more things, as part of the compilation step:
   1. **Loss function:** How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.
   2. **Optimizer:** The mechanism through which the network will update itself based on the data it sees and its loss function.
   3. **Metrics** Here, we'll only care about accuracy (the fraction of the images that were correctly classified).

* While we are already familiar with defining metrics to optimize, defining an optimizer and loss function is new. We will dig into that later. 
* Notice that the `compile()` function modifies the network in place. We will talk about all of them later in a bit more detail.

```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

Lets inspect our final setup:

```{r}
summary(network)
```

Well' we see that a network of this size has quite a large number of trainable parameters (all edge-weights, meaning 512x512 + 512x10).

## Preprocess the data

* Before training the model, preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the `[0, 1]` interval. 
* Previously, our training images were stored in an 3d array of shape `(60000, 28, 28)` of type integer with values in the `[0, 255]` interval. 
* We transform it into a double array of shape `(60000, 28 * 28)` with values between `0` and `1`.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255 # To scale between 0 and 1

test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255 # To scale between 0 and 1
```


* Note that we use the `array_reshape()` rather than the `dim()` function to reshape the array. I explain why later, when we talk about **tensor reshaping**.
* Lastly, we also need to categorically encode the labels.

```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

## Run the network

We're now ready to train the network via `Keras` `fit()` function. We save the output in an object we call `history.net`. 

```{r}
set.seed(1337)
history.net <- network %>% fit(x = train_images, 
                               y = train_labels, 
                               epochs = 10, # How often shall we re-run the model on the whole sample
                               batch_size = 128, # How many observations should be included in every batch
                               validation_split = 0.25 # If we want to do a  cross-validation in the training
                               )
```

* Two quantities are displayed in the log during training: the loss and accuracy of the network over the training data during the subsequent epochs (new training runs after re-adjusting the weights). 
* Notice that the measures improve in every epoch. We quickly reach an accuracy (98.9% on the training data. 
* Notice that `fit()` adjusts the weights of the network without explicly assigning it into a new object. `history.net` therefore only contains the history of the models prediction metrics through the different epochs, in case we would like to inspect it. Let's take a look:

```{r}
history.net
```

```{r}
history.net %>% glimpse()
```

We can also visualize these metrics through the epocs.

```{r}
history.net %>% plot(smooth = TRUE)
```

* Interestingly, we already see that our model overfits. Meaning, while accuracy in our training set tends to further increase through the epocs, it starts over time to decrease in our validation set. T
* here are different ways to fight that, such as defining a `layer_dropout`, or to tell the model to pick stop running further epocs as soon as the validation accuracy drops. However, we will for now just move on.
* For now, let's check if the model performs well out-of-sample on the test-set:

```{r}
metrics <- network %>% evaluate(test_images, test_labels)
```

```{r}
metrics
```

* Ok, so far so good. I think that's a decent accuracy for such an ad-hoc model. Whith a bit of tinkering, we surely could get it to 99%. But thats a task for another time...
* Lets go back to basic and revise a bit what we have done so far.

# Data representations for neural networks

* In the previous example, we started from data stored in multidimensional arrays, also called **tensors**. 
* In general, most current ML systems use tensors as their basic data structure. Tensors are fundamental to the field-so fundamental that Google's **TensorFlow** was named after them. So what's a tensor?
* Tensors are a generalization of vectors and matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis). 
* In `R`, vectors are used to create and manipulate 1D tensors, and matrices are used for 2D tensors. For higher-level dimensions, array objects (which support any number of dimensions) are used.

## Key tensor-attributes

A tensor is defined by three key attributes:

1. **Number of axes** (rank): For instance, a 3D tensor has three axes, and a matrix has two axes.
2. **Shape:** This is an integer vector that describes how many dimensions the tensor has along each axis. 
3. **Data type:** This is the type of the data contained in the tensor; for instance, a tensor's type could be integer or double. On rare occasions, you may see a character tensor. 

## Tensor reshaping

Remember that we before did not use the `dim()` but the `array_reshape()` function to manipulate our input tensors.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
```

```{r}
str(train_images)
```

```{r}
dim(train_images)
```

* This is an `R` specific thingy, so that the data is reinterpreted using row-major semantics (as opposed to `R`s default column-major semantics), which is in turn compatible with the way the numerical libraries called by `Keras` (`NumPy`, `TensorFlow`, and so on) interpret array dimensions. 
* You should always use the `array_reshape()` function when reshaping `R` arrays that will be passed to `Keras`.
* Reshaping a tensor means rearranging its rows and columns to match a target shape. 
* Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. Lets do a simple examples:

```{r}
x <- matrix(c(0:5),
            nrow = 3, ncol = 2, byrow = TRUE)
x
```

```{r}
x <- array_reshape(x, dim = c(3, 2))
x
```

```{r}
x <- array_reshape(x, dim = c(2, 3))
x
```

* A special case of reshaping that's commonly encountered is *transposition*. 
* Transposing a matrix means exchanging its rows and its columns, so that `x[i,]` becomes `x[, i]`. The `t()` function can be used to transpose a matrix:


```{r}
x <- t(x)
x
```

```{r}
rm(x)
```


## Geometric interpretation of tensor operations

```{r,eval=FALSE}
layer <- layer_dense(units = 32, input_shape = c(784))
```

* We're creating a layer that will only accept as input 2D tensors where the first dimension is 784 (the first dimension, the batch dimension, is unspecified, and thus any value would be accepted). This layer will return a tensor where the first dimension has been transformed to be 32.
* Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don't have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. 
* For instance, suppose you write the following:

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = c(784)) %>%
  layer_dense(units = 32)
```

```{r}
model
```

```{r}
# devtools::install_github("andrie/deepviz")
library(deepviz)
plot_model(model)
```

* The second layer didn't receive an input shape argument-instead, it automatically inferred its input shape as being the output shape of the layer that came before.

* Picking the right network architecture is more an art than a science; and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. 
* Here, we will simit ourself to a simple feed-forward network, where every layer is only connected to the following one. For now, there are two key architecture decisions to be made about such a stack of dense layers:
   1. How many layers to use?
   2. How many hidden units to choose for each layer?
   3. Which activation function to use?

```{r}
rm(layer, model)
```


## Activation functions

* As we already saw, we can define activation functiopns for the different activation functions for every layer. While we within the intermediate layers seldomely switch between different activation functions, the one we define for the output layer critically depends on the shape of our desired output data.
* A brief reminder: Activation functions transform the input weights of a cell to its output. Without them, the dense layer would consist of two linear operations-a dot product and an addition:
* In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. 
   * `relu` is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: `prelu`, `elu`, and so on. A `relu` (rectified linear unit) is a function meant to zero out negative values, and commonly used for intermediate layers (formerly, almost all layers where modelled with `sigmoid`, but nowadays its proven that for intermediate layers `relu` mostly works better).

![](https://sds-aau.github.io/SDS-master/00_media/DL_activation_1.jpg){width=500px}

Our output layer, however, should model a binary choice (yes/no classification). For such a model, we would in a 2-class problem commonly use a a `sigmoid` function, which we already know from logistic regression models. It "squashes" arbitrary values into the `[0, 1]` interval, outputting something that can be interpreted as a probability.

![](https://sds-aau.github.io/SDS-master/00_media/DL_activation_2.jpg){width=500px}

However, since we have a multi-class prediction problem, we choose `softmax`, which squashes the outputs of each unit to be between `0` and `1`, just like a `sigmoid`, but it also divides each output such that the total sum of the outputs is equal to `1`. The output is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.

If you are interested regarding the different types of layers in `Keras`, check [the reference site](https://keras.rstudio.com/reference/index.html) with all layers implemented. Furthermore, types of activation functions are discussed [HERE](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)


## Loss functions and optimizers: keys to configuring the learning process

Once the network architecture is defined, you still have to choose two more things:

* **Loss function** (objective function): The quantity that will be minimized during training. It represents a measure of success for the task at hand.
* **Optimizer:** Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).

* Choosing the right objective and function for the right problem is extremely important: your network will take any shortcut it can, to minimize the loss; so if the objective doesn't fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. 

* [HERE](https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718) you find a brief overview on different loss functions. Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. 
* Take this rule-of-thumb table as a good starter:

![](https://sds-aau.github.io/SDS-master/00_media/DL_table_ann_config.png){width=750px}

* With respect to the `optimizer`: We will cover that later. There are a bunch of different around, most variants of the **Stochastic Gradient Descent (SGD)**, **Batch (vanilla) Gradient Descent**, and **Mini-Batch Gradient Descent**. 
* [HERE](https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6) and [HERE](http://ruder.io/optimizing-gradient-descent/) you find a nice summary for the interested reader that wants to now more. Currently (that might change soon, since everything in DL movers fast), its common knowledge that if you have nmo strong reasons to believe so, `RMSprop` (an unpublished, adaptive learning rate method proposed by Geoff Hinton) with standard learning rates works just well.

# Reviewing our initial example

Let's go back to the first example and review each piece of it in the light of what we have learned up to now:  This was the input data:

```{r,eval=FALSE}
mnist <- dataset_mnist()

train_images <- mnist$train$x
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255

test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
```


* Now you understand that the input images are stored in tensors of shape `(60000, 784)` (training data) and `(10000, 784)` (test data), respectively.
* This was our network:

```{r,eval=FALSE}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

* Now you understand that this network consists of a chain of two dense layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. 
* We know that `layer_dense()` creates fully connected layers, so there exists a weight between every element of one with every element of the following layer.
* Weight tensors, which are attributes of the layers, are where the knowledge of the network persists. We know the 2nd layer has `512` cells, the final output layer `10` (equal to the number of classes to predict). Finally, we know that every cell also contains a non-linear activation function, such as `relu`, `sigmoid`, or `softmax`.

This was the network-compilation step:

```{r,eval=FALSE}
network network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = "accuracy"
  )
```

* Now you understand that `categorical_crossentropy` (a measure how pure the predicted classes are) is a type of a `loss`` function that's used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. 
* You also know that this reduction of the loss happens via mini-batch stochastic gradient descent. The exact rules governing a specific use of gradient descent are defined by the `rmsprop` optimizer passed as the first argument.

Finally, this was the training loop:

```{r,eval=FALSE}
network %>% fit(x = train_images, 
                y = train_labels, 
                epochs = 10, 
                batch_size = 128)
```


* Now you understand what happens when you call `fit()`: the network will start to iterate on the training data in mini-batches of 128 samples, 10 times over (each iteration over all the training data is called an ?epoch?). 
* At each iteration, the network will compute the gradients of the weights with regard to the loss on the batch, and update the weights accordingly. 
* After these ?10? epochs, the network will have performed ?2,345? gradient updates (?469? per ?epoch?), and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy.

At this point, you already know most of what there is to know about the basics of neural networks.

However, there is still some stuff to come, namely:

1. How to use architectures other that the simple **feed-forward** one.
2. How to fight overfitting
3. How to specify training routines and parameter grid-search
4. And some more...

But for that, there will be other sessions top come...

# Example on tabular data

* Just to make it a bit lss abstract and work with tabular data, lets give it a shot with classifying penguins :)

## Load data

```{r}
library(tidymodels)
```

```{r}
data <- read_csv("https://github.com/allisonhorst/palmerpenguins/raw/5b5891f01b52ae26ad8cb9755ec93672f49328a8/data/penguins_size.csv")
```

```{r}
data %>% glimpse()
```

## TRain TEst split
```{r}
data %<>%
  rename(y = species_short) %>%
  relocate(y) %>%
  drop_na()
```

```{r}
data_split <- tidymodels::initial_split(data, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```

## Preprocessing recipe

```{r}
data_recipe <- data_train %>%
  recipe(y ~.) %>%
  step_center(all_numeric(), -all_outcomes()) %>% # Centers all numeric variables to mean = 0
  step_scale(all_numeric(), -all_outcomes()) %>% # scales all numeric variables to sd = 1
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()
```

```{r}
x_train <- juice(data_recipe) %>% select(-starts_with('y')) %>% as.matrix()
x_test <- bake(data_recipe, new_data = data_test) %>% select(-starts_with('y')) %>% as.matrix()
```

```{r}
y_train <- juice(data_recipe)  %>% select(starts_with('y')) %>% as.matrix()
y_test <- bake(data_recipe, new_data = data_test) %>% select(starts_with('y')) %>% as.matrix()
```

# Define the network

```{r}
model_keras <- keras_model_sequential()
```

```{r}
model_keras %>% 
  # First hidden layer
  layer_dense(
    units              = 12, 
    activation         = "relu", 
    input_shape        = ncol(x_train)) %>% 
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  # Second hidden layer
  layer_dense(
    units              = 12, 
    activation         = "relu") %>% 
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  # Output layer
  layer_dense(
    units              = ncol(y_train), 
    activation         = "softmax") 
```

```{r}
model_keras %>% 
  compile(
    optimizer = "adam",
    loss = "categorical_crossentropy",
    metrics = "accuracy"
  )
```

```{r}
model_keras_hist <- model_keras  %>% fit(x = x_train, 
                                         y = y_train, 
                                         epochs = 10, # How often shall we re-run the model on the whole sample
                                         batch_size = 12, # How many observations should be included in every batch
                                         validation_split = 0.25 # If we want to do a  cross-validation in the training
                                         )
```

# Your Turn

Using the Keras API and reference ([Python](https://keras.io/) or [R](https://keras.rstudio.com/)) manual and in particular construct additional simple models (with appropriate metrics):

* Regression example
* Multi-label example

Also consider further resources mentioned below

# Endnotes

### References

### More info

You can find more info about:

* `keras` [here](https://keras.rstudio.com/): Excellent documentation, tutorials, and resources regarding `keras`, maintained by Rstudio

### Online Resources

Datacamp
   * [Introduction to TensorFlow in R](https://learn.datacamp.com/courses/introduction-to-tensorflow-in-r): A bit low-level, but a good intro for starters
   * Also follow the Python intros, they might still be helpful for you.
   
Others

* [RStudio AI blog](https://blogs.rstudio.com/ai/): Excellent source for frequent torch/keras exercises and announcements within th R ecosystem
* [R Markdown Notebooks for "Deep Learning with R"](https://github.com/skeydan/deep-learning-with-r-notebooks): A collection of exercises/tutorials/demos from the [Deep Learning with R](https://www.manning.com/books/deep-learning-with-r) book. Good illustrations of diffent types of ML problems and their solutions.

Books

* François Chollet & J. J. Allaire (2018). Deep Learning with R, Manning Publications: Good book, but not for free. Find it [here](https://www.manning.com/books/deep-learning-with-r). though, in case of interest.


### Session info

```{r}
sessionInfo()
```