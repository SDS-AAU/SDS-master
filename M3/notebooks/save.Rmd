---
title: "R Notebook"
output: html_notebook
---


# Introduction to Neural Networks and Deep Learning 

## AI & Co: What's that?

![](https://sds-aau.github.io/SDS-master/00_media/random_cylon.jpg){width=750px}

In the past few years, artificial intelligence (AI) has been a subject of intense media hype. Machine learning, deep learning, and AI come up in countless articles, often outside of technology-minded publications. We're promised a future of intelligent chatbots, self-driving cars, and virtual assistants-a future sometimes painted in a grim light and other times as utopian, where human jobs will be scarce, and most economic activity will be handled by robots or AI agents. For a future or current practitioner of machine learning, it's important to be able to recognize the signal in the noise so that you can tell world-changing developments from overhyped press releases. 

So, lets first delineate the space, and clarify the terminology.

![](https://sds-aau.github.io/SDS-master/00_media/dl_overview.png){width=500px}

In fact, we will realize, that machine learning is just a particular subset of the realm of AI technologies and algorithms. Artificial Neural Networks in turn are just a subset of ML techniques. In fact, we already used them briefly in M1. Deep Learning, again, is a subsey of ML, refering to a particular type of neural networks.

## Artificial Neural Networks

### The general idea

The term neural network is a reference to neurobiology, but although some of the central concepts in deep learning were developed in part by drawing inspiration from our understanding of the brain, deep-learning models are not models of the brain. They share some very basics, but are highly stilized. Its like saying a paper airplane is an artificial F20 fighter jet. Anyhow, lets look at the general idea:

Somewhat like the brain, we can model decision processes somewhat like that:

* We take input cells (neurons, in that case), and connect them to some output cell (in neuroscience, via a synapse). 
* The recieving cell's input is equal to the submitting cell's output, weighted by the strenght of the connection. 
* The cell transforms this input via a non-linear activation function to an output, which it in turn submits to connected cells. 

The simplest of such toy models is called a **perceptron**:

![](https://sds-aau.github.io/SDS-master/00_media/DL_perceptron.png){width=500px}

Obviously, the flexibility of the funtional form a perceptron can model is pretty limited. However, that changes quickly by adding an adittional layer, and some hidden cells in between, which can be understood as latent variables.

![](https://sds-aau.github.io/SDS-master/00_media/DL_neural_net3.png){width=750px}

### A little illustration:

Before we dive into the deep, lets start with exploring a shallow neural network. That we just can do in the well-known `caret` workflow, deploying the `nnet` package.

```{r}
library(caret)
library(nnet)
```

We will finally use the famous `iris` dataset, where we aim at predicting species of different iris flowers by their sepal and pedal length and width.

```{r}
library(datasets)
data <- iris
```

```{r}
data %>% glimpse()
```

We do the usual stuff, create a train and test split, and fit a neural net.

```{r}
index <- createDataPartition(y = data$Species, p = 0.75, list = FALSE)
train <- data[index,] 
test <- data[-index,] 

fit.nnet <- train(Species ~ ., train, 
              method='nnet', 
              trace = FALSE)

fit.nnet
```

We can take a look at the hyperparameter tuning under the hood.

```{r}
plot(fit.nnet)
```

The tunable parameters here are the number of hidden units (cells in layer 2), and the weight decay (how fast the model unlearns, a measure to counter overfitting). More on that lateron.

Lets see how good it predicts on the test data.

```{r}
table(predict(fit.nnet, test[,1:4]), test$Species)
```

Booohja, we got it pretty much all right! 

We can also plot the structure of the neural network to get a first intuition on what's happening there.

```{r,fig.height=7.5,fig.width=12.5}
library(NeuralNetTools)
plotnet(fit.nnet$finalModel, alpha=0.6)
```

We see the variables enter as an input layer, sepperated by a latent (hidden) layer from the output. The model iteratively tunes the weights between them in order to best fit input to output. But ok, lets move on... this was just a warm up.

```{r,echo=FALSE}
rm(list=ls())
```


### The *deep* in deep learning

So, whats the special thing about **deep learning**, and why is it **deep anyhow**? Conceptually, DL is a new take on learning representations from data that puts an emphasis on learning **successive layers of increasingly meaningful representations**, which is almost exclusively done via a neural network architecture.  In contrast, most other ML approaches focus on learning only one or two layers of representations of the data $\rightarrow$ shallow learning. 

What do the representations learned by a deep-learning algorithm look like? Let's examine how a network several layers deep transforms an image of a digit in order to recognize what digit it is.

![](https://sds-aau.github.io/SDS-master/00_media/dl_concept_1.jpg){width=750px}

the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).

![](https://sds-aau.github.io/SDS-master/00_media/DL_concept_2.jpg){width=750px}

So that's what deep learning is, technically: a multistage way to learn data representations. It's a simple idea-but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic.




