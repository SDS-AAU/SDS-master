---
title: "R Notebook"
output: html_notebook
---


# Introduction to Neural Networks and Deep Learning 

## AI & Co: What's that?

![](https://sds-aau.github.io/SDS-master/00_media/random_cylon.jpg){width=750px}

In the past few years, artificial intelligence (AI) has been a subject of intense media hype. Machine learning, deep learning, and AI come up in countless articles, often outside of technology-minded publications. We're promised a future of intelligent chatbots, self-driving cars, and virtual assistants-a future sometimes painted in a grim light and other times as utopian, where human jobs will be scarce, and most economic activity will be handled by robots or AI agents. For a future or current practitioner of machine learning, it's important to be able to recognize the signal in the noise so that you can tell world-changing developments from overhyped press releases. 

So, lets first delineate the space, and clarify the terminology.

![](https://sds-aau.github.io/SDS-master/00_media/dl_overview.png){width=500px}

In fact, we will realize, that machine learning is just a particular subset of the realm of AI technologies and algorithms. Artificial Neural Networks in turn are just a subset of ML techniques. In fact, we already used them briefly in M1. Deep Learning, again, is a subsey of ML, refering to a particular type of neural networks.

## Artificial Neural Networks

### The general idea

The term neural network is a reference to neurobiology, but although some of the central concepts in deep learning were developed in part by drawing inspiration from our understanding of the brain, deep-learning models are not models of the brain. They share some very basics, but are highly stilized. Its like saying a paper airplane is an artificial F20 fighter jet. Anyhow, lets look at the general idea:

Somewhat like the brain, we can model decision processes somewhat like that:

* We take input cells (neurons, in that case), and connect them to some output cell (in neuroscience, via a synapse). 
* The recieving cell's input is equal to the submitting cell's output, weighted by the strenght of the connection. 
* The cell transforms this input via a non-linear activation function to an output, which it in turn submits to connected cells. 

The simplest of such toy models is called a **perceptron**:

![](https://sds-aau.github.io/SDS-master/00_media/DL_perceptron.png){width=500px}

Obviously, the flexibility of the funtional form a perceptron can model is pretty limited. However, that changes quickly by adding an adittional layer, and some hidden cells in between, which can be understood as latent variables.

![](https://sds-aau.github.io/SDS-master/00_media/DL_neural_net3.png){width=750px}

### A little illustration:

Before we dive into the deep, lets start with exploring a shallow neural network. That we just can do in the well-known `caret` workflow, deploying the `nnet` package.

```{r}
library(caret)
library(nnet)
```

We will finally use the famous `iris` dataset, where we aim at predicting species of different iris flowers by their sepal and pedal length and width.

```{r}
library(datasets)
data <- iris
```

```{r}
data %>% glimpse()
```

We do the usual stuff, create a train and test split, and fit a neural net.

```{r}
index <- createDataPartition(y = data$Species, p = 0.75, list = FALSE)
train <- data[index,] 
test <- data[-index,] 

fit.nnet <- train(Species ~ ., train, 
              method='nnet', 
              trace = FALSE)

fit.nnet
```

We can take a look at the hyperparameter tuning under the hood.

```{r}
plot(fit.nnet)
```

The tunable parameters here are the number of hidden units (cells in layer 2), and the weight decay (how fast the model unlearns, a measure to counter overfitting). More on that lateron.

Lets see how good it predicts on the test data.

```{r}
table(predict(fit.nnet, test[,1:4]), test$Species)
```

Booohja, we got it pretty much all right! 

We can also plot the structure of the neural network to get a first intuition on what's happening there.

```{r,fig.height=7.5,fig.width=12.5}
library(NeuralNetTools)
plotnet(fit.nnet$finalModel, alpha=0.6)
```

We see the variables enter as an input layer, sepperated by a latent (hidden) layer from the output. The model iteratively tunes the weights between them in order to best fit input to output. But ok, lets move on... this was just a warm up.

```{r,echo=FALSE}
rm(list=ls())
```






### The *deep* in deep learning

So, whats the special thing about **deep learning**, and why is it **deep anyhow**? Conceptually, DL is a new take on learning representations from data that puts an emphasis on learning **successive layers of increasingly meaningful representations**, which is almost exclusively done via a neural network architecture.  In contrast, most other ML approaches focus on learning only one or two layers of representations of the data $\rightarrow$ shallow learning. 

What do the representations learned by a deep-learning algorithm look like? Let's examine how a network several layers deep transforms an image of a digit in order to recognize what digit it is.

![](https://sds-aau.github.io/SDS-master/00_media/dl_concept_1.jpg){width=750px}

the network transforms the digit image into representations that are increasingly different from the original image and increasingly informative about the final result. You can think of a deep network as a multistage information-distillation operation, where information goes through successive filters and comes out increasingly purified (that is, useful with regard to some task).

![](https://sds-aau.github.io/SDS-master/00_media/DL_concept_2.jpg){width=750px}

So that's what deep learning is, technically: a multistage way to learn data representations. It's a simple idea-but, as it turns out, very simple mechanisms, sufficiently scaled, can end up looking like magic.


# Introduction to `Keras`

There are quite a bunch of deep learning frameworks around, from the older `Caffee` and `Theano` to Google's `Tensorflow` and the newer `Pytorch`. However, during the rest of this course, 95% of our deep learning exercises will be done using `Keras`
Keras is a deep-learning framework that provides a convenient way to define and train almost any kind of deep-learning model. Keras was initially developed for researchers, with the aim of enabling fast experimentation. 

It has the following advantages:

* User-friendly API which makes it easy to quickly prototype deep learning models.
* Built-in support for convolutional networks (for computer vision), recurrent networks (for sequence processing), and any combination of both.
* Supports arbitrary network architectures: multi-input or multi-output models, layer sharing, model sharing, etc., is therefore appropriate for building essentially any deep learning model, from a memory network to a neural Turing machine.
* Is capable of running on top of multiple back-ends including `TensorFlow`, `CNTK`, or `Theano`.
* Allows the same code to run on CPU or on GPU, and has strong multi-GPU, distributed storage, and training support (`Google cloud`, `Spark`, `HDF5`...)
* Can easily be integrated in AI products (Apple `CoreML`, `TensorFlow` Android runtime,  `R` or `Python` webapp backend such as a `Shiny` or `Flask` app)

It is widely adapted in academia and industry (Google, Netflix, Uber, CERN, Yelp, Square etc.), and is also a popular framework on Kaggle, the machine-learning competition website, where almost every recent deep-learning competition has been won using `Keras` models. While Google's `TensorFlow` is even more popular, keep in mind that `Keras` can use `Tensorflow` (and other popular DL frameworks) as backend, and allows less cumbersome and more high-level 


<img width="49%" src="https://sds-aau.github.io/SDS-master/00_media/dl_frameworks_1.jpg"/>
<img width="49%" src="https://sds-aau.github.io/SDS-master/00_media/dl_frameworks_2.png"/>

So, after all, `Keras` represents a wonderful high-level starter, fast and easy implementable, and in most cases flexible enough to do whatever you feel like.

![](https://sds-aau.github.io/SDS-master/00_media/DL_keras_wtf.png){width=750px}

**Sidenote:** The weird name (`Keras`) means *horn* in Greek, and is a reference to ancient Greek literature. Eg., in Odyssey, supernatural *dream spirits* are divided between those who deceive men with false visions (arriving to Earth through a gate of ivory), and those who announce a future that will come to pass (arriving through a gate of horn). So, enough history lessons, let's run our first deep learning model!



# Our first deep learning model

## Introduction

Well, its about time to get serious. We will dive straight in, and use a simple deep learning model on the classical `Mnist` dataset. This is the original data used by Jan LeCun and his team to fit an ANN that identifies handwritten digits for the US postal service. It consists of quite a bunch of samples of handwritten dicites together with their correct label. The wandwritten dicits here conveniently come as a 28x28 greyscale matrix, making them a good starter to warm up. Lets do that.


## Load our data and get ready

```{r}
# Load our main tool
library(keras)

# Load our data
mnist <- dataset_mnist()
```

```{r}
saveRDS(mnist, file = "mnist.RDS")
```


```{r}
# sepperate in train and test
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

Lets take a look at the structure.

```{r}
glimpse(train_images)
```

```{r}
glimpse(train_labels)
```


## Define the `Keras` model

The workflow will be as follows: 

1. First, we'll feed the neural network the training data, `train_images` and `train_labels`. 
2. The network will then learn to associate images and labels. 
3. Finally, we'll ask the network to produce predictions for `test_images`, and we'll verify whether these predictions match the labels from `test_labels`.

Let's build the network - again, remember that you aren't expected to understand everything about this example yet.

Building a model in `Keras` that can be fitted on your data involves two steps:

1. Defining the networks architecture interms of layers and their shape.
2. Compiling the model, and defining the loss function, evaluation metric, and optimizer.

```{r}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

Notice that the layer stacking in `R` is done via the well-known `%>%`, in `Pyhton` with `.`. That's about the main difference between both implementations. 

The core building block of neural networks is the **layer**, a data-processing module that you can think of as a filter for data. Some data goes in, and it comes out in a more useful form. Specifically, layers extract representations out of the data fed into them - hopefully, representations that are more meaningful for the problem at hand. Most of deep learning consists of chaining together simple layers that will implement a form of progressive data distillation. 

Here, our network consists of a sequence of two layers, which are **densely connected** (`layer_dense`) neural layers. The second (and last) layer is a 10-way `softmax` layer, which means it will return an array of 10 probability scores (summing to 1). Each score will be the probability that the current digit image belongs to one of our 10 digit classes. So, we defined a network with overall 634 cells, consisting of:

1. input layer: 28x28 = 512 cells
2. intermediate layer : 28x28  = 512 cells
3. Output layer: 10 cells

To make the network ready for training, we need to pick three more things, as part of the compilation step:

1. **Loss function:** How the network will be able to measure its performance on the training data, and thus how it will be able to steer itself in the right direction.
2. **Optimizer:** The mechanism through which the network will update itself based on the data it sees and its loss function.
3. **Metrics** Here, we'll only care about accuracy (the fraction of the images that were correctly classified).

While we are already familiar with defining metrics to optimize, defining an optimizer and loss function is new. We will dig into that later. Notice that the `compile()` function modifies the network in place. We will talk about all of them later in a bit more detail.

```{r}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = c("accuracy")
)
```

Lets inspect our final setup:

```{r}
summary(network)
```

Well' we see that a network of this size has quite a large number of trainable parameters (all edge-weights, meaning 512x512 + 512x10).

## Preprocess the data

Before training the model, preprocess the data by reshaping it into the shape the network expects and scaling it so that all values are in the `[0, 1]` interval. Previously, our training images were stored in an 3d array of shape `(60000, 28, 28)` of type integer with values in the `[0, 255]` interval. We transform it into a double array of shape `(60000, 28 * 28)` with values between `0` and `1`.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255 # To scale between 0 and 1

test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255 # To scale between 0 and 1
```


Note that we use the `array_reshape()` rather than the `dim()` function to reshape the array. I explain why later, when we talk about **tensor reshaping**.

Lastly, we also need to categorically encode the labels.

```{r}
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
```

## Run the network

We're now ready to train the network via `Keras` `fit()` function. We save the output in an object we call `history.net`. 

```{r}
history.net <- network %>% fit(x = train_images, 
                               y = train_labels, 
                               epochs = 10, # How often shall we re-run the model on the whole sample
                               batch_size = 128, # How many observations should be included in every batch
                               validation_split = 0.25 # If we want to do a  cross-validation in the training
                               )
```

Two quantities are displayed in the log during training: the loss and accuracy of the network over the training data during the subsequent epochs (new training runs after re-adjusting the weights). Notice that the emasures improve in every epoch. We quickly reach an accuracy (98.9% on the training data. 

Notice that `fit()` adjusts the weights of the network without explicly assigning it into a new object. `history.net` therefore only contains the history of the models prediction metrics through the different epochs, in case we would like to inspect it. Let's take a look:

```{r}
history.net
```

```{r}
history.net %>% glimpse()
```

We can also visualize these metrics through the epocs.

```{r}
history.net %>% plot(smooth = TRUE)
```

Interestingly, we already see that our model overfits. Meaning, while accuracy in our training set tends to further increase through the epocs, it starts over time to decrease in our validation set. There are different ways to fight that, such as defining a `layer_dropout`, or to tell the model to pick stop running further epocs as soon as the validation accuracy drops. However, that we will save for later sessions. 

For now, let's check if the model performs well out-of-sample on the test-set:

```{r}
metrics <- network %>% evaluate(test_images, test_labels)
```

```{r}
metrics
```

Ok, so far so good. I think that's a decent accuracy for such an ad-hoc model. Whith a bit of tinkering, we surely could get it to 99%. But thats a task for another time...

Lets go back to basic and revise a bit what we have done so far.


# Data representations for neural networks

In the previous example, we started from data stored in multidimensional arrays, also called **tensors**. In general, most current ML systems use tensors as their basic data structure. Tensors are fundamental to the field-so fundamental that Google's **TensorFlow** was named after them. So what's a tensor?

Tensors are a generalization of vectors and matrices to an arbitrary number of dimensions (note that in the context of tensors, a dimension is often called an axis). In `R`, vectors are used to create and manipulate 1D tensors, and matrices are used for 2D tensors. For higher-level dimensions, array objects (which support any number of dimensions) are used.

## Key tensor-attributes

A tensor is defined by three key attributes:

1. **Number of axes** (rank): For instance, a 3D tensor has three axes, and a matrix has two axes.
2. **Shape:** This is an integer vector that describes how many dimensions the tensor has along each axis. 
3. **Data type:** This is the type of the data contained in the tensor; for instance, a tensor's type could be integer or double. On rare occasions, you may see a character tensor. But because tensors live in preallocated contiguous memory segments, and strings, being variable-length, would preclude the use of this implementation, they're rarely used.

To make this more concrete, let's look back at the data we processed in the MNIST example. Since we already manipulated it, we reload it again in its original shape:

```{r}
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
```

Now we display the number of axes of the tensor `train_images`, and then its shape and datatype:

```{r}
dim(train_images)
```

```{r}
typeof(train_images)
```

So what we have here is a 3D tensor of integers. More precisely, it's an array of 60,000 matrices of 28x28 integers. Each such matrix is a grayscale image, with coefficients between 0 and 255. Thats how they look:

```{r}
digit <- train_images[5,,]
digit[,8:20] # I crop it a bit, otherwise the columns dont fit on one page
```

To make it more tangible, lets plot one:

```{r}
digit %>% as.raster(max = 255) %>% plot()
```

## Tensors and dimensionality

![](https://sds-aau.github.io/SDS-master/00_media/dl_tensors_funny.jpg){width=750px}

### Scalars (0D tensors)

A tensor that contains only one number is called a scalar (or scalar tensor, or zero-dimensional tensor, or 0D tensor). ?R? doesn't have a data type to represent scalars (all numeric objects are vectors, matrices, or arrays), but an R vector that's always length `1` is conceptually similar to a scalar.

```{r}
x <- 1
```

```{r}
x
```



### Vectors (1D tensors)

A one-dimensional array of numbers is called a vector, or 1D tensor. A 1D tensor is said to have exactly one axis. We can convert the `R` vector to an array object to inspect its dimensions:

```{r}
x <- c(12, 3, 6, 14, 10)
```

```{r}
str(x)
```

```{r}
x %>% as.array() %>% dim()
```


This vector has five entries and so is called a five-dimensional vector. Don't confuse a 5D vector with a 5D tensor! A 5D vector has only one axis and has five dimensions along its axis, whereas a 5D tensor has five axes (and may have any number of dimensions along each axis). Dimensionality can denote either the number of entries along a specific axis (as in the case of our 5D vector) or the number of axes in a tensor (such as a 5D tensor), which can be confusing at times. In the latter case, it's technically more correct to talk about a tensor of rank 5 (the rank of a tensor being the number of axes), but the ambiguous notation 5D tensor is common regardless.

### Matrices (2D tensors)

A two-dimensional array of numbers is a matrix, or 2D tensor. A matrix has two axes (often referred to as rows and columns). You can visually interpret a matrix as a rectangular grid of numbers:

```{r}
x <- matrix(rep(0, 3*5), nrow = 3, ncol = 5)
```

```{r}
x
```

```{r}
dim(x)
```


### Arrays (3D and higher-dimensional tensors)

If you pack such matrices in a new array, you obtain a 3D tensor, which you can visually interpret as a cube of numbers:

```{r}
x <- array(rep(0, 3*5*2), 
           dim = c(3,5,2))
```

```{r}
x
```


```{r}
str(x)
```

```{r}
dim(x)
```


By packing 3D tensors in an array, you can create a 4D tensor, and so on. In deep learning, you'll generally manipulate tensors that are 0D to 4D, although you may go up to 5D if you process video data.

For example, in the case before, we where working with 3d tensors, where the first two where a (greyscale pixel) matrix, and the third the different observations (samples) stacked on each others. 

3d tensors are also often used for time series. Whenever time matters in your data (or the notion of sequence order), it makes sense to store it in a 3D tensor with an explicit time axis. Each sample can be encoded as a sequence of vectors (a 2D tensor), and thus a batch of data will be encoded as a 3D tensor.

![](https://sds-aau.github.io/SDS-master/00_media/DL_tensor_3d.jpg){width=500px}

However, this greyscale raster matrix is somewhat a special case. Images typically have three dimensions: height, width, and color. Therefore, a 2d image would therefore still represent a 3d tensor, and a bunch of them together a 4d tensor. For example, a batch of 128 RGB-color images could be stored in a 4d tensor of shape `(128, 256, 256, 3)`

![](https://www.dropbox.com/s/9qrf22g6zo1mydn/DL_tensor_4d.jpg){width=500px}

You might already see it coming.... vidoes represent a time series of images, therefore would be a 5d tensor. Videos can be seen as series of frames, where each frame can be stored in a 3D tensor `(height, width, color_depth)`,  their sequence in a 4D tensor `(frames, height, width, color_depth)`, and thus a batch of different videos  in a 5D tensor of shape `(samples, frames, height, width, color_depth)`.


## Tensor reshaping

Remember that we before did not use the `dim()` but the `array_reshape()` function to manipulate our input tensors.

```{r}
train_images <- array_reshape(train_images, c(60000, 28 * 28))
```

```{r}
str(train_images)
```

```{r}
dim(train_images)
```

This is an `R` specific thingy, so that the data is reinterpreted using row-major semantics (as opposed to `R`s default column-major semantics), which is in turn compatible with the way the numerical libraries called by `Keras` (`NumPy`, `TensorFlow`, and so on) interpret array dimensions. You should always use the `array_reshape()` function when reshaping `R` arrays that will be passed to `Keras`.

Reshaping a tensor means rearranging its rows and columns to match a target shape. Naturally, the reshaped tensor has the same total number of coefficients as the initial tensor. Lets do a simple examples:

```{r}
x <- matrix(c(0, 1,
              2, 3,
              4, 5),
            nrow = 3, ncol = 2, byrow = TRUE)
x
```

```{r}
x <- array_reshape(x, dim = c(3, 2))
x
```

```{r}
x <- array_reshape(x, dim = c(2, 3))
x
```

A special case of reshaping that's commonly encountered is *transposition*. Transposing a matrix means exchanging its rows and its columns, so that `x[i,]` becomes `x[, i]`. The `t()` function can be used to transpose a matrix:


```{r}
x <- t(x)
x
```

## Geometric interpretation of tensor operations

Because the contents of the tensors manipulated by tensor operations can be interpreted as coordinates of points in some geometric space, all tensor operations have a geometric interpretation. For instance, let's consider addition. 

We'll start with the following vector: `A = [0.5, 1.0]`. It's a point in a 2D space, but can also be under stood as a vector leading from the origin to this point.

![](https://sds-aau.github.io/SDS-master/00_media/DL_vector_1.jpg){width=300px}

Let's consider a new point, `B = [1, 0.25]`, which we willll add to the previous one. This is done geometrically by chaining together the vector arrows, with the resulting location being the vector representing the sum of the previous two vectors

![](https://sds-aau.github.io/SDS-master/00_media/DL_vector_2.jpg){width=300px}

In general, elementary geometric operations such as affine transformations, rotations, scaling, and so on can be expressed as tensor operations. For instance, a rotation of a 2D vector by an angle `theta` can be achieved via a dot product with a 2x2 matrix `R = [u, v]`, where `u` and `v` are both vectors of the plane: `u = [cos(theta), sin(theta)]` and `v = [-sin(theta), cos(theta)]`.

Some linear, vector and matrix algebra knowledge might light up again in your brain, right? Good! While for many out-of-the box "run-this-model" operations on tabular data, you might not need it, it will be necessary in case you need to tinker and customize a bit at your models to squeeze out a bit more accuracy. 

![](https://sds-aau.github.io/SDS-master/00_media/DL_linear_algebra.jpg){width=500px}

Anyhow, a bit of a refresher in linear algebra would also support your intuition on what's going on under the hood of deep learning (Btw: there is a DataCamp course on that). We, however, leave it like that for now.



# Learning in Neural Networks

## The learning problem

So, now we know what the *deep* means, and how such a deep neutwork is structured. However, we still do not know so much about the *learning*. Generally, learning appears in the network by adjusting the weights between the different cells. But how does that happen?

Lets take first a step back. Every cell gets `inputs` from the connected other cells on lower layers which are activated, where the intensity of the input is scaled by the weight of the connection. If the cell gets activated on its own is determined by its **activation function**, a mathematical transformation of its inputs, where the cell (usually) activates above a certain threshhold. This can be done in different ways, eg, a **rectified linear unit** (ReLU) or a **sigmoid**, which we already know from logistic regression models

Back to our network: Here, each neural layer from our first network example transforms its input data as follows:

`output = relu(dot(W, input) + b)`

In this expression, `W` and `b` are tensors that are attributes of the layer. They're called the weights or trainable parameters of the layer (the kernel and bias attributes, respectively). These weights contain the information learned by the network from exposure to training data. The activation of every cell in the layer is therefore dependent on the multiplication of the corresponding input and weight tensor (`dot(W, input)`), pluss the bias (`b`), a constant which influences the tendency to activate.

Initially, these weight matrices are filled with small random values (a step called *random initialization*). Of course, there is no reason to expect that `relu(dot(W, input) + b)`, when `W` and `b` are random, will yield any useful representations. What comes next is to gradually adjust these weights, based on some feedback signal (provided by the *loss function dicussed later*). 

This gradual adjustment, also called *training*, is basically the learning that ML is all about. This happens within what's called a training loop, which works as follows. Repeat these steps in a loop, as long as necessary:

1. Draw a batch of training samples `x` and corresponding targets `y`.
2. Run the network on `x` (a step called the *forward pass*) to obtain predictions `y_pred`.
3. Compute the **loss** of the network on the batch, a measure of the mismatch between `y_pred` and `y`.
4. Update all weights of the network in a way that slightly reduces the loss on this batch.

You'll eventually end up with a network that has a very low loss on its training data: a low mismatch between predictions `y_pred` and expected targets `y`. The network has "learned" to map its inputs to correct targets. From afar, it may look like magic, but when you reduce it to elementary steps, it turns out to be simple.

Step 1 sounds easy enough-just I/O code. Steps 2 and 3 are merely the application of a handful of tensor operations, so you could implement these steps purely from what you learned in the previous section. 

The difficult part is step 4: updating the network's weights. Given an individual weight coefficient in the network, how can you compute whether the coefficient should be increased or decreased, and by how much?

The currently dominnat approach to do so is to take advantage of the fact that all operations used in the network are differentiable, and compute the *gradient of the loss* with regard to the network's coefficients. You can then move the coefficients in the opposite direction from the gradient, thus decreasing the loss.

## Refresher What's a derivative?

Consider a continuous, smooth function `f(x) = y`, mapping a real number `x` to a new real number `y`. Because the function is continuous, a small change in x can only result in a small change in `y`. Let's say you increase `x` by a small factor `epsilon_x`: this results in a small `epsilon_y` change to `y`:

`f(x + epsilon_x) = y + epsilon_y`

In addition, because the function is smooth (its curve has no abrupt angles), when `epsilon_x` is small enough, around a certain point `p`, it's possible to approximate `f` as a linear function of slope `a`, so that `epsilon_y` becomes `a * epsilon_x`:

`f(x + epsilon_x) = y + a * epsilon_x`

Obviously, this linear approximation is valid only when `x` is close enough to `p`. The slope `a` is called the **derivative** of `f` in `p`. 

![](https://sds-aau.github.io/SDS-master/00_media/DL_gradient_1.jpg){width=400px}

For every differentiable function `f(x)`, there exists a derivative function `f'(x)` that maps values of `x` to the slope of the local linear approximation of `f` in those points. If you're trying to update `x` by a factor `epsilon_x` in order to minimize `f(x)`, and you know the derivative of `f`, then your job is done: the derivative completely describes how `f(x)` evolves as you change `x`. 

If you want to reduce the value of `f(x)`, you just need to move `x` a little in the opposite direction from the derivative. It is helpful to track cregions where `f'(x)==0`, since they indicate the directional change of the curvature, and therefore local maxima or minima of `f(x)`.

Btw: The curvature of `f(x)` is found in its second derivative, `f''(x)`.

## Derivative of a tensor operation: the gradient

A **gradient** is the derivative of a tensor operation. Consider an input vector `x`, a matrix `W`, a target `y`, and a loss function (to be explained later) `loss`. You can use `W` to compute a target candidate `y_pred`, and compute the `loss`, or mismatch, between the target candidate `y_pred` and the target `y`:

`y_pred = dot(W, x)`
`loss_value = loss(y_pred, y)`

If the data inputs `x` and `y` are frozen, then this can be interpreted as a function mapping values of `W` to `loss` values:

`loss_value = f(W)`

### Stochastic gradient descent
Since `f'(x)==0`indicates a local minimum, to find the global one we "only" need to identify all `f'(x)==0` regions and check which has the lowest `f(x)`. Applied to a neural network, that means finding analytically the combination of weight values that yields the smallest possible `loss`. 

![](https://sds-aau.github.io/SDS-master/00_media/DL_gradient_2.jpg?dl=1){width=400px}

To do so, we use the four-step algorithm outlined earlier: modify the parameters little by little based on the current loss value on a random batch of data. Because you're dealing with a differentiable function, you can compute its gradient, which gives you an efficient way to implement step 4. 

If you update the weights in the opposite direction from the gradient, the `loss` will be a little less every time:

1. Draw a batch of training samples `x` and corresponding targets `y`.
2. Run the network on `x` to obtain predictions `y_pred`.
3. Compute the loss of the network on the batch, a measure of the mismatch between `y_pred` and `y`.
4. Compute the gradient of the `loss` with regard to the network's parameters (a backward pass).
5. Move the parameters a little in the opposite direction from the gradient-for example, `W = W - (step * gradient)` - thus reducing the `loss` on the batch a bit.

Easy enough! What we just described is **stochastic gradient descent** (mini-batch SGD). The term stochastic refers to the fact that each batch of data is drawn at random (stochastic is a scientific synonym of random). 

![](https://www.dropbox.com/s/5q6sxn3wuzyp8uz/DL_gradient_3.jpg?dl=1){width=400px}

As you can see, intuitively it's important to pick a reasonable value for the `step`, which we call the **learning rate**. If it's too small, the descent down the curve will take many iterations, and it could get stuck in a local minimum. If step is too large, your updates may end up taking you to completely random locations on the curve.

There are many different ways to tinkwer with things like the learning rate and momentum (we call these ways of defining the learning function to find the global optimum *optimizer*), which influence how efficient the process leads (or not) to a global optimum.

![](https://www.dropbox.com/s/84gqqwtrg2knjkj/DL_optimizer.gif?dl=1){width=400px}

### The Backpropagation algorithm: Chaining derivatives

So, now we are almost there. We know now how to minimize the `loss function` within a layer. However, the issue with deep learning is... well... that you want to sue **many** layers. 

How do we move on from here? Indeed, the rise of deep learning had to wait till the implementation of a efficient way to train multi-layered networks.

Luckily, we now have it, and its called **backpropagation**. And its actually pretty simple. While an enourmeous task in terms of number of calculations, the math behind it chould be acessible by highschool students. Just imagine, we have a 4-layered network, connected by 4 weight tensors.

`f(W1, W2, W3) = a(W1, b(W2, c(W3)))`

Calculus tells us that such a chain of functions can be differentiated using the following identity, called the **chain rule** (ohh, dark memories, right?): 

`f(g(x)) = f'(g(x)) * g'(x)`

Applying the chain rule to the computation of the gradient values of a neural network was one simple but brillinat ideas. Now we can start with the last output layer, and propagate the loss via the chain rule step-by-step back over all weights in the leyer below, and then the one below etc., and adjust the weights accordingly. 

Backpropagation starts with the final loss value and works backward from the top layers to the bottom layers, applying the chain rule to compute the contribution that each parameter had in the loss value.

Sounds like a hell to calculate by hand, right? Would be, but dont worry, you will not have to.


# The parts of a deep learning model

## Overview

As we understand by now training a neural network revolves around the following objects:

1. The **layers**, which are combined into a network (or model)
2. The **input** data and corresponding **outcome** targets
3. The **loss function**, which defines the feedback signal used for learning
4. The **optimizer**, which determines how learning proceeds


In interaction, it can be illustrated like this: The network, composed of layers that are chained together, maps the input data to outcomes of interest by doing predictions. The loss function then compares these predictions to the targets, producing a loss value: a measure of how well the network's predictions match what was expected. The optimizer uses this loss value to update the network's weights.

![](https://www.dropbox.com/s/pvop4cm45k5rscu/DL_deep_learning_parts.jpg?dl=1){width=750px}

Let's take a closer look at layers, networks, loss functions, and optimizers.

## Layers: the building blocks of deep learning

A layer is a data-processing module that takes as input one or more tensors and that outputs one or more tensors. Some layers are stateless, but more frequently layers have a state: the layer's weights, one or several tensors learned with stochastic gradient descent, which together contain the network's knowledge.

Different layers are appropriate for different tensor formats and different types of data processing. For instance, simple vector data, stored in 2D tensors of shape `(samples, features)`, is often processed by **densely connected layers**, also called fully connected or dense layers (the `layer_dense` function in Keras). Sequence data, stored in 3D tensors of shape (samples, timesteps, features), is typically processed by recurrent layers such as `layer_lstm`. Image data, stored in 4D tensors, is usually processed by 2D convolution layers (`layer_conv_2d`). All that will be introduced in later sessions.

You can think of layers as the LEGO bricks of deep learning, a metaphor that is made explicit by frameworks like `Keras`. Building deep-learning models in Keras is done by clipping together compatible layers to form useful data-transformation pipelines. The notion of layer compatibility here refers specifically to the fact that every layer will only accept input tensors of a certain shape and will return output tensors of a certain shape. Consider the following example:

```{r,eval=FALSE}
layer <- layer_dense(units = 32, input_shape = c(784))
```

We're creating a layer that will only accept as input 2D tensors where the first dimension is 784 (the first dimension, the batch dimension, is unspecified, and thus any value would be accepted). This layer will return a tensor where the first dimension has been transformed to be 32.

Thus this layer can only be connected to a downstream layer that expects 32-dimensional vectors as its input. When using Keras, you don't have to worry about compatibility, because the layers you add to your models are dynamically built to match the shape of the incoming layer. For instance, suppose you write the following:

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 32, input_shape = c(784)) %>%
  layer_dense(units = 32)
```

```{r}
model
```

```{r}
# devtools::install_github("andrie/deepviz")
library(deepviz)
plot_model(model)
```

The second layer didn't receive an input shape argument-instead, it automatically inferred its input shape as being the output shape of the layer that came before.

Picking the right network architecture is more an art than a science; and although there are some best practices and principles you can rely on, only practice can help you become a proper neural-network architect. Indeed, the community so far developed a multitude of different architectures, all more or less suitable for a certain task and data structure. Again,the most important ones will be introduced in later sessions.

![](https://www.dropbox.com/s/qqx8kib1avwcb40/DL_ann_zoo.jpg?dl=1){width=750px}

Here, we will simit ourself to a simple feed-forward network, where every layer is only connected to the following one. For now, there are two key architecture decisions to be made about such a stack of dense layers:

1. How many layers to use?
2. How many hidden units to choose for each layer?
3. Which activation function to use?


## Activation functions

As we already saw, we can define activation functiopns for the different activation functions for every layer. While we within the intermediate layers seldomely switch between different activation functions, the one we define for the output layer critically depends on the shape of our desired output data.

A brief reminder: Activation functions transform the input weights of a cell to its output. Without them, the dense layer would consist of two linear operations-a dot product and an addition:

`output = dot(W, input) + b`

So the layer could only learn linear transformations (affine transformations) of the input data: the hypothesis space of the layer would be the set of all possible linear transformations of the input data. Such a hypothesis space is too restricted and wouldn't benefit from multiple layers of representations, because a deep stack of linear layers would still implement a linear operation: adding more layers wouldn't extend the hypothesis space.

In order to get access to a much richer hypothesis space that would benefit from deep representations, you need a non-linearity, or activation function. 

`relu` is the most popular activation function in deep learning, but there are many other candidates, which all come with similarly strange names: `prelu`, `elu`, and so on. A `relu` (rectified linear unit) is a function meant to zero out negative values, and commonly used for intermediate layers (formerly, almost all layers where modelled with `sigmoid`, but nowadays its proven that for intermediate layers `relu` mostly works better).

![](https://www.dropbox.com/s/jeebmvvcptg4cly/DL_activation_1.jpg?dl=1){width=500px}

Our output layer, however, should model a binary choice (yes/no classification). For such a model, we would in a 2-class problem commonly use a a `sigmoid` function, which we already know from logistic regression models. It "squashes" arbitrary values into the `[0, 1]` interval, outputting something that can be interpreted as a probability.

![](https://www.dropbox.com/s/whm8dcug8r6fyad/DL_activation_2.jpg?dl=1){width=500px}

However, since we have a multi-class prediction problem, we choose `softmax`, which squashes the outputs of each unit to be between `0` and `1`, just like a `sigmoid`, but it also divides each output such that the total sum of the outputs is equal to `1`. The output is equivalent to a categorical probability distribution, it tells you the probability that any of the classes are true.

If you are interested regarding the different types of layers in `Keras`, check [the reference site](https://keras.rstudio.com/reference/index.html) with all layers implemented. Furthermore, types of activation functions are discussed [HERE](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)

## Loss functions and optimizers: keys to configuring the learning process
Once the network architecture is defined, you still have to choose two more things:

* **Loss function** (objective function): The quantity that will be minimized during training. It represents a measure of success for the task at hand.
* **Optimizer:** Determines how the network will be updated based on the loss function. It implements a specific variant of stochastic gradient descent (SGD).

Choosing the right objective and function for the right problem is extremely important: your network will take any shortcut it can, to minimize the loss; so if the objective doesn't fully correlate with success for the task at hand, your network will end up doing things you may not have wanted. Imagine a stupid, omnipotent AI trained via SGD, with this poorly chosen objective function: "maximizing the average well-being of all humans alive." To make its job easier, this AI might choose to kill all humans except a few and focus on the well-being of the remaining ones-because average well-being isn't affected by how many humans are left. That might not be what you intended! Just remember that all neural networks you build will be just as ruthless in lowering their loss function-so choose the objective wisely, or you'll have to face unintended side effects.

[HERE](https://towardsdatascience.com/understanding-different-loss-functions-for-neural-networks-dd1ed0274718) you find a brief overview on different loss functions. Fortunately, when it comes to common problems such as classification, regression, and sequence prediction, there are simple guidelines you can follow to choose the correct loss. Take this rule-of-thumb table as a good starter:

![](https://www.dropbox.com/s/kqu036aa3prq9vj/DL_table_ann_config.png?dl=1){width=750px}

With respect to the `optimizer`: We will cover that later. There are a bunch of different around, most variants of the **Stochastic Gradient Descent (SGD)**, **Batch (vanilla) Gradient Descent**, and **Mini-Batch Gradient Descent**. 

[HERE](https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6) and [HERE](http://ruder.io/optimizing-gradient-descent/) you find a nice summary for the interested reader that wants to now more. Currently (that might change soon, since everything in DL movers fast), its common knowledge that if you have nmo strong reasons to believe so, `RMSprop` (an unpublished, adaptive learning rate method proposed by Geoff Hinton) with standard learning rates works just well.

# Reviewing our initial example

Let's go back to the first example and review each piece of it in the light of what we have learned up to now:  This was the input data:

```{r,eval=FALSE}
mnist <- dataset_mnist()

train_images <- mnist$train$x
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255

test_images <- mnist$test$x
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
```


Now you understand that the input images are stored in tensors of shape `(60000, 784)` (training data) and `(10000, 784)` (test data), respectively.

This was our network:

```{r,eval=FALSE}
network <- keras_model_sequential() %>%
  layer_dense(units = 512, activation = "relu", input_shape = c(28*28)) %>%
  layer_dense(units = 10, activation = "softmax")
```

Now you understand that this network consists of a chain of two dense layers, that each layer applies a few simple tensor operations to the input data, and that these operations involve weight tensors. We know that `layer_dense()` creates fully connected layers, so there exists a weight between every element of one with every element of the following layer.

Weight tensors, which are attributes of the layers, are where the knowledge of the network persists. We know the 2nd layer has `512` cells, the final output layer `10` (equal to the number of classes to predict). Finally, we know that every cell also contains a non-linear activation function, such as `relu`, `sigmoid`, or `softmax`.

This was the network-compilation step:

```{r,eval=FALSE}
network %>% compile(
  optimizer = "rmsprop",
  loss = "categorical_crossentropy",
  metrics = "accuracy"
  )
```

Now you understand that ?categorical_crossentropy? (a measure how pure the predicted classes are) is a type of a ?loss? function that's used as a feedback signal for learning the weight tensors, and which the training phase will attempt to minimize. You also know that this reduction of the loss happens via mini-batch stochastic gradient descent. The exact rules governing a specific use of gradient descent are defined by the ?rmsprop? optimizer passed as the first argument.

Finally, this was the training loop:

```{r,eval=FALSE}
network %>% fit(x = train_images, 
                y = train_labels, 
                epochs = 10, 
                batch_size = 128)
```


Now you understand what happens when you call `fit()`: the network will start to iterate on the training data in mini-batches of ?128? samples, ?10? times over (each iteration over all the training data is called an ?epoch?). At each iteration, the network will compute the gradients of the weights with regard to the ?loss? on the ?batch?, and update the weights accordingly. After these ?10? epochs, the network will have performed ?2,345? gradient updates (?469? per ?epoch?), and the loss of the network will be sufficiently low that the network will be capable of classifying handwritten digits with high accuracy.

At this point, you already know most of what there is to know about the basics of neural networks.

However, there is still some stuff to come, namely:

1. How to use architectures other that the simple **feed-forward** one.
2. How to fight overfitting
3. How to specify training routines and parameter grid-search
4. And some more...

But for that, there will be other sessions top come...

### Your turn

Please do **Exercise 1** in the corresponding section on `Github`. 

A known dataset but new architecture awaits you!



# Endnotes

### References

### More info
You can find more info about:

* `keras` [here](https://keras.rstudio.com/)

### Session info
```{r}
sessionInfo()
```


