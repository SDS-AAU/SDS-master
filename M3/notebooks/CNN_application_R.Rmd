---
title: 'Neural Networks Application: Convolutional Neural Networks for Image data (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(magrittr)

library(keras)
```

# Intro to Convolutional Neural Networks and Computer Vision

## On Cats, Dogs and Hotdogs

In this notebook you will learn about the different building blocks that form a convolutional neural net as well as how we can build one using Keras. CNNs are the kind of neural networks that really require computational resources and therefore, you should consider using Colab/Kaggle with GPU (or TPU support if you can figure it out) support to run that. If you run it on your own computer without a GPU things will take a lot of time...like a lot!

## Getting the data

```{r}
# Let's start by downloading and exploring the data
temp <- tempfile()
download.file('https://storage.googleapis.com/sds-file-transfer/dataset.zip',temp)
unzip(temp)
unlink(temp)
```

```{r}
list.files(path = "dataset", include.dirs = TRUE) 
```

```{r}
list.files(path = "dataset/training_set") %>% head()
```

The data is actually a folder with 3 folders inside it. A *training_set*, a *test_set* and another one for try-outs

In each the training and test_set folders we have 2 folders again. One for cats and one for dogs.

```{r}
list.files(path = "dataset/training_set/cats") %>% head()
list.files(path = "dataset/training_set/dogs") %>% head()
```

Here some examples:

![](https://source.unsplash.com/7AIDE8PrvA0/200x300)
![](https://source.unsplash.com/h7VBJRBcieM/200x300)

Now think, you are a computer and need to classify that. :-O

# Preprocessing image data

* Now the "data-engineering" part, which i find a bit tricky.
* Our input are a bunch of jpeg images with cats and dogs.
* Obviousely (I hope), we are not going to load the images into memory with pandas or something like that. Rather we are going to stream them during training one batch at a time. 
* However, we cannot just throw a jpeg at the network. That wouldn't be nice. We need to transform the images to matrices on the fly. 

## Formatting & streaming the data

I first define a few other parameters in the beginning to make adapting as easy as possible.

```{r}
# list of fruits to modle
class_list <- c('cats', 'dogs')

# number of output classes (i.e. fruits)
output_n <- length(class_list)

# image size to scale down to (original images are 100 x 100 px)
img_width <- 64
img_height <- 64
target_size <- c(img_width, img_height)

# RGB = 3 channels
channels <- 3

# path to image folders
train_files_path <- 'dataset/training_set'
test_iles_path <- 'dataset/test_set'
```





## Image augmentation

Another thing that we also will do is "image augmentation". 

> Image Augmentations techniques are methods of artificially increasing the variations of images in our data-set by using horizontal/vertical flips, rotations, variations in brightness of images, horizontal/vertical shifts etc.

You can read more on that and in general about generators [here](https://medium.com/@arindambaidya168/https-medium-com-arindambaidya168-using-keras-imagedatagenerator-b94a87cdefad).

![](https://cdn-images-1.medium.com/max/1600/1*rZRYWg0ve6bZv2-ctEtVXg.png)

![](https://cdn-images-1.medium.com/max/1600/1*0aMp3TW3rxCUL1JFmeJj9Q.png)
* The handy `image_data_generator()` and `flow_images_from_directory()` functions can be used to load images from a directory. 
* If you want to use data augmentation, you can directly define how and in what way you want to augment your images

```{r}
# optional data augmentation
train_data_gen = image_data_generator(
  rescale = 1/255, #,
  shear_range = 0.2,
  zoom_range = 0.2,
  horizontal_flip = TRUE,
  #fill_mode = "nearest",
  #rotation_range = 40,
  #width_shift_range = 0.2,
  #height_shift_range = 0.2
)

# Validation data shouldn't be augmented! But it should also be scaled.
test_data_gen <- image_data_generator(
  rescale = 1/255
  )  
```

Now we load the images into memory and resize them.

```{r}
# training images
train_image_array_gen <- flow_images_from_directory(train_files_path, 
                                          train_data_gen,
                                          target_size = target_size,
                                          class_mode = "binary",
                                          classes = class_list,
                                          batch_size = 32,
                                          seed = 1337)

# validation images
test_image_array_gen <- flow_images_from_directory(test_files_path, 
                                          test_data_gen,
                                          target_size = target_size,
                                          class_mode = "binary",
                                          classes = class_list,
                                          batch_size = 32,
                                          seed = 1337)
```

```{r}
table(factor(train_image_array_gen$classes))
```

* We now want to save the class indicies in order to be able to match it with the predictions later

```{r}
train_image_array_gen$class_indices
```

```{r}
classes_indices <- train_image_array_gen$class_indices
```

# Defining the model

```{r}
model <- keras_model_sequential() 
```

## Model Architecture

### Step 1: Adding a convolutional layer
```{r}
# Step 1 - Convolution - This is new
model %>%
  layer_conv_2d(filter = 32, 
                kernel_size = c(3,3), 
                padding = "same", 
                input_shape = c(img_width, img_height, channels),
                activation = 'relu') 
```

* We use 32 different filters that will be built as 3x3 matrices. We also specify that our input shape is 64x64x3, meaning that we have 3 matrices (for RGB) of 64 pixels each side.


First, we should perhaps get an overall picture of how a CNN architecture looks.

![alt text](https://cdn-images-1.medium.com/max/2000/1*w5peCK-AeSI9D0PRT8oiZw.png)

### Step 2: Add MAxPooling

```{r}
model %>%
  layer_max_pooling_2d(pool_size = c(2,2)) 
```

* Adding that layer requires just to specify the size of the "pool" - and we are done. 
* Now, let's check out [something fun:](http://scs.ryerson.ca/~aharley/vis/conv/)

### Repeat :)

* We add to more layers of the same structure.

```{r}
model %>%
  layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = "same",  input_shape = c(img_width, img_height, channels), activation = 'relu') %>%
  layer_max_pooling_2d(pool_size = c(2,2)) 
```

### Step 3: Flattening

```{r}
# Step 3 - Flattening
model %>%
   layer_flatten()
```

* This layer is easy: Take all pooled features and line them up in one long vector, then convatenate.

### Step 4: Dense layer

* Finally: We add a "regular" artificial neural net including a bit of dropout (not really needed but why not)

```{r}
model %>%
  layer_dense(units = 128, activation = 'relu') %>%
  layer_dropout(rate = 0.2)
```


### Output Layer

* The final layer has a sigmoid activation function due to the binary classification problem.

```{r}
model %>%
  layer_dense(units = 1, activation = 'sigmoid') 
```

* Lets se what we finally got.

```{r}
model %>% summary()
```

```{r}
# deepviz::plot_model(model) # Visualize if you like
```

## Compile 

```{r}
# compile
model %>% compile(
  loss = "binary_crossentropy",
  optimizer = 'adam',
  metrics = "accuracy"
)
```

## Train the network

# And now we can train the network

```{r}
# And now we can train the network
hist <- model %>% fit_generator(
  train_image_array_gen,
  steps_per_epoch = 800,
  epochs = 2, 
  validation_data = test_image_array_gen,
  validation_steps = 100
  )
```

```{r}
hist %>% plot()
```


```{r}
model %>% evaluate_generator(test_image_array_gen, steps = 500)
```





