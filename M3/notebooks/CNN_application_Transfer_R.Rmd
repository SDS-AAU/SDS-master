---
title: 'Neural Networks Application: Transfer learning with CNNs (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(magrittr)
library(keras)
```

# Using large pretrained models as your foundation

* This notebook is based on https://www.learnopencv.com/keras-tutorial-fine-tuning-using-pre-trained-models/
* Training large CNNs is costly (compute, time) and often we don't have enough data to bring these models to reasonable performance levels. This is where transfer-learning comes in.
* Assuming that the early layers of a CNN learn very basic features (edges, lines etc.) we can consider a strategy, where we take a large pretrained model, discard the last couple of layers (where we would assume the more abstract information) and train it with our new data.
* This is how you, for instance, could fine-tune a CNN model to some very specific medical imaging data where you simply cannot have so many examples.

# Load the data

It's the same cats&dog data...

```{r}
# Let's start by downloading and exploring the data
temp <- tempfile()
download.file('https://storage.googleapis.com/sds-file-transfer/dataset.zip',temp)
unzip(temp)
unlink(temp)
```

# Preprocessing

Same as in the former notebook

```{r}
class_list <- c('cats', 'dogs')
output_n <- length(class_list)
img_width <- 64
img_height <- 64
target_size <- c(img_width, img_height)
channels <- 3
train_files_path <- 'dataset/training_set'
test_files_path <- 'dataset/test_set'
```

```{r}
train_data_gen = image_data_generator(rescale = 1/255, shear_range = 0.2, zoom_range = 0.2, horizontal_flip = TRUE)
test_data_gen <- image_data_generator(rescale = 1/255)  
```

```{r}
train_image_array_gen <- flow_images_from_directory(train_files_path, train_data_gen, target_size = target_size, class_mode = "binary", 
                                                    classes = class_list,batch_size = 32, seed = 1337)

test_image_array_gen <- flow_images_from_directory(test_files_path, test_data_gen, target_size = target_size, class_mode = "binary",
                                          classes = class_list, batch_size = 32, seed = 1337)
```

# Loading pretrained model

Keras has some great built-in applications - basically pretrained models with some nice functional overhead. We will start by loadeing VGG16 (a rather large model with many many many layers that has been trained on imagenet)


```{r}
# create the base pre-trained model
base_model <- application_vgg16(
  include_top = FALSE,
  weights='imagenet',
  input_shape = c(img_height, img_width, channels))
```

* Now we got the model, where we can add some own layers



```{r}
predictions <- base_model$output %>% 
  layer_global_average_pooling_2d() %>% 
  layer_dense(units = 1024, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'sigmoid')
```


```{r}
# this is the model we will train
model <- keras_model(inputs = base_model$input, outputs = predictions)
```

*The trick is, to "freeze" most of the initial layers and thereby preserve the majority of the information embedded in the model already. 
* We will only train the added new dense layer as well as an output layer that fit's our purpose with the model - finding cats and dogs

```{r}
# first: train only the top layers 
for (layer in base_model$layers)
  layer$trainable <- FALSE
```

```{r}
# compile the model (should be done *after* setting layers to non-trainable)
model %>% compile(optimizer = 'adam', 
                  loss = 'binary_crossentropy',
                  metric = 'accuracy')
```

 
```{r}
# train the model on the new data for a few epochs
model %>% fit_generator(
  train_image_array_gen,
  steps_per_epoch = 800,
  epochs = 2, 
  verbose = FALSE,
  )
```

* at this point, the top layers are well trained and we can start fine-tuning convolutional layers. 
* We will freeze the bottom N layers nd train the remaining top layers. 

```{r}
# let's visualize layer names and layer indices to see how many layers we should freeze:
layers <- base_model$layers
for (i in 1:length(layers))
  cat(i, layers[[i]]$name, "\n")
```

```{r}
# we chose to train the top 2 inblocks, i.e. we will freeze the first layers and unfreeze the rest:
for (i in 1:172)
  layers[[i]]$trainable <- FALSE
for (i in 173:length(layers))
  layers[[i]]$trainable <- TRUE
```
 
```{r}
# we need to recompile the model for these modifications to take effect
model %>% compile(optimizer = 'adam', 
                  loss = 'binary_crossentropy',
                  metric = 'accuracy')
```

```{r}
# we train our model again (this time fine-tuning the top 2  blocks
# alongside the top Dense layers
# train the model on the new data for a few epochs
model %>% fit_generator(
  train_image_array_gen,
  steps_per_epoch = 800,
  epochs = 2, 
  verbose = FALSE,
  )
```

# Evaluation returns loss and accuracy

```{r}
model %>% evaluate_generator(test_image_array_gen, steps=500)
```


 

 

 

