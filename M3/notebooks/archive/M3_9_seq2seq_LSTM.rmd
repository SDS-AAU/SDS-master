---
title: 'M3-9: Sequence-2-Sequence forecasting (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation
set.seed(1337) # To have a seed defined for reproducability

### Knitr options
if (!require("knitr")) install.packages("knitr"); library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

### Install packages if necessary
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               magrittr # For advanced piping (%>% et al.)
               )
```


# LSTMs for Forecasting

Time series prediction has experienced dramatic improvements in predictive accuracy as a result of the data science machine learning and deep learning evolution. As these ML/DL tools have evolved, businesses and financial institutions are now able to forecast better by applying these new technologies to solve old problems. Particularly, LSTMs have here proven useful for problems involving sequences with autocorrelation. 

Particularly challenging are predictions over multiple periods. When predictions are based on predictions, which are based on predictions, small errors amplify in every step. Furthermore, time series data that follows certain trends, such as stock market data, tends to follow cyclic trends. That means, knowing the current state of your future prediction target are not helpful without information on the current trend based on a series of previous states. Obviously, LSTMs are made for exactly such tasks. Lets get started!

For this exercise, we need a couple of adittional packages, particularly for working with time-series data, and doing the (somewhat messy) sampling for time-series train-control workflows:

```{r}
# others
library(tidyquant)

# Time Series
library(forecast)
library(tsibble)

# Preprocessing
library(recipes)

# Sampling / Accuracy
library(rsample)
library(yardstick) 
```

We will in the following take up two tasks:

1. A Many-to-one prediction of the next value of a timeseries given its past states.
2. A Many-to_many forecast of a future timeseries given its past states (outlook).

## The `Sunspot` Dataset

`Sunspots` is a famous data set included in `R` . The dataset tracks sunspots, which are the occurrence of a dark spot on the sun. Here’s an image from NASA showing the solar phenomenon. Pretty cool!

![](https://www.dropbox.com/s/pjrqhaulc9rnzqa/random_sunspot_nasa.jpg?dl=1){width=750px}

We now load `sunspot.month`. It’s a `ts` class for timeseries data (not tidy), which is used by many oldschool forecasting packages such as `forecast`.

```{r}
sun_spots <- datasets::sunspot.month
```

```{r}
sun_spots %>% glimpse()
```

We see that the time series of recorder sunspots indeed starts in 1749, so we have more than 250 years of data. Cool!

```{r}
sun_spots %>% head(24)

```

```{r}
sun_spots %>% tail(24)
```

We see the data is recorded monthly.

Note that `ts` is not a tidy format. While useful with many of the oldschool timeseries packages, for our further workflow we would like to have a tidy timeseries instead. Here, we will use the `as_tsibble()` function to confert the `ts` into a tidy timeseries object `tsibble` from the package with the same name.

```{r}
sun_spots_tidy <- sun_spots %>%
  as_tsibble()
```

```{r}
sun_spots_tidy 
```
```{r}
min(sun_spots_tidy$index)
```

```{r}
max(sun_spots_tidy$index)
```


```{r}
#sun_spots_tidy %<>%
#  filter(lubridate::year(index) >= 1752 & lubridate::year(index) <= 2012)
```




## Data Exploration & Visualization

### Visualizing sunspot frequency over time

When `forecasts` is loaded, it directly delivers  `autoplot` (plotting of a standard style for a specific data-object) instructions to `ggplot2`, which can be just called.

```{r, fig.width=12.5,fig.height=5}
autoplot(sun_spots)
```

This already gives us an idea that sunspot pattern indeed seem to be highly cyclic, with ups and downs round about every 10 years. This is good news for now!

While this is nice and convenient, we can do much better on our own using some `ggplot2` basics on the tidy `tsibble`. 

```{r, fig.width=12.5,fig.height=5}
# Just for the optics, I will use the pretty theme and color palette from the tidyquant package
sun_spots_tidy %>%
  ggplot(aes(index, value)) +
  geom_point(color = tidyquant::palette_light()[[1]], alpha = 0.5) +
  tidyquant::theme_tq() +
    labs(title = "Sunspots", 
         subtitle = "From 1749 to 2013 (Full Data Set)")
```

We can also zoom in a bit into a 50 year period to get a better glance of variations within the cycles.

```{r, fig.width=12.5,fig.height=5}
sun_spots_tidy %>%
    filter(lubridate::year(index) <= 1800) %>%
    ggplot(aes(index, value)) +
    geom_line(color = tidyquant::palette_light()[[1]], alpha = 0.5) +
    geom_point(color = tidyquant::palette_light()[[1]]) +
    geom_smooth(method = "loess", span = 0.2, se = FALSE) +
    tidyquant::theme_tq() +
    labs(title = "Sunspots", 
         subtitle = "1749 to 1800 (Zoomed In To Show Cycle)")

```

At first glance, it looks like this series should be easy to predict. However, we can see that the cycle (10-year frequency) and amplitude (count of sunspots) seems to change at least between years 1780 and 1800. This creates some challenges.

### Evaluating the Autocorrelation Function (AFC)

The next thing we can do is determine whether or not an LSTM model may be a good approach. The LSTM based *only on past values* of sunspots will leverage **autocorrelation** to generate sequence predictions. 

Our final goal is to produce a 10-year forecast using batch forecasting (a technique for creating a single forecast batch across the forecast region, which is in contrast to a single-prediction that is iteratively performed one or several steps into the future). The batch prediction will only work if the autocorrelation used is beyond ten years. Let’s inspect.

First, we need to review the Autocorrelation Function (ACF), which is the correlation between the time series of interest in lagged versions of itself. The `acf()` function from the `stats` library returns the ACF values for each lag as a plot. 

```{r, fig.width=12.5,fig.height=5}
# This function does not work with tidydata, so we use the original `ts()` object instead.
sun_spots %>% ggAcf()
```


This is good news. We have autocorrelation in excess of 0.5 beyond lagged values (in years) of more than 15 years . We can theoretically use one of the high autocorrelation lags to develop an LSTM model. Lets `filter` for the optimal lag (in month).

##  Predictions

### Getting the data ready

#### Development of a Backtesting Strategy

Cross validation is the process of developing models on sub-sampled data against a validation set with the goal of determining an expected accuracy level and error range. Time series is a bit different than non-sequential data when it comes to cross validation. Specifically, the time dependency on previous time samples must be preserved when developing a sampling plan. 

We can create a cross validation sampling plan using by offsetting the window used to select sequential sub-samples. In finance, this type of analysis is often called **Backtesting**, which takes a time series and splits it into multiple uninterupted sequences offset at various windows that can be tested for strategies on both current and past observations.

A recent development is the `rsample` package, which makes cross validation sampling plans very easy to implement. Further, the `rsample` package (as part of the `tidymodels` ecosystem) has Backtesting covered. The `vignette`, “Time Series Analysis Example”, describes a procedure that uses the `rolling_origin()` function to create samples designed for time series cross validation. We’ll use this approach.


![](https://www.dropbox.com/s/8kc3k1euwgprox0/DL_LSTM_walkforward2.png?dl=1)

The sampling plan we create uses:

* 50 years (`initial = 12 * 50` samples) for the training set 
* 10  years (`assess = 12 * 10`) for the testing (validation) set 

We select a skip span of twenty years (`skip = 12 * 20`) to evenly distribute the samples into sets that span the entire 260 years of sunspots history. Last, we select `cumulative = FALSE` to allow the origin to shift which ensures that models on more recent data are not given an unfair advantage (more observations) that those on less recent data. The tibble return contains the `rolling_origin_resamples`.

```{r}
periods_train <- 12 * 50 # Training 
periods_test  <- 12 * 10 # Test
skip_span     <- 12 * 20 # How 

rolling_origin_resamples <- rolling_origin(
    sun_spots_tidy,
    initial    = periods_train, # The number of samples used for analysis/modeling in the initial resample.
    assess     = periods_test, # The number of samples used for each assessment resample.
    cumulative = FALSE, #  A logical. Should the analysis resample grow beyond the size specified by initial at each resample?
    skip       = skip_span # A integer indicating how many (if any) additional resamples to skip to thin the total amount of data points in the analysis resample.
)
```

```{r}
rolling_origin_resamples
```


```{r}
glimpse(rolling_origin_resamples)
```

```{r}
glimpse(rolling_origin_resamples$splits[[1]])
```



## A statefull LSTM in `Keras`

```{r}
library(keras)
# Sys.setenv(RETICULATE_PYTHON="/usr/local/share/.virtualenvs/r-reticulate/bin/") # Only necessary in kaggle currently
```

To begin, we’ll develop a single Keras Stateful LSTM model on a single sample from the Backtesting Strategy. We’ll then scale the model to all samples to investigate/validate the modeling performance.

### LSTM for singe prediction

For the single LSTM model, we’ll select and visualize the split for the most recent time sample/slice.

```{r}
split    <- rolling_origin_resamples$splits[[11]]
split_id <- rolling_origin_resamples$id[[11]]
```


#### Data Setup

First, let’s combine the training and testing data sets into a single data set with a column key that specifies what set they came from (either `training` or `testing`). 

```{r}
df_train <- training(split) %>% mutate(key = "training")
df_test <- testing(split) %>% mutate(key = "testing")

df <- rbind(df_train, df_test) # Note: Use rbind and not bind_rows, which for some reason drops the time indexing
```

```{r}
df
```


#### Preprocessing
The LSTM algorithm requires the input data to be centered and scaled. We can preprocess the data using the `recipes` package, as we did in M1. We’ll use a combination of `step_sqrt` to transform the data and reduce the presence of outliers and `step_center` and `step_scale` to center and scale the data. 

```{r}
rec_obj <- recipe(value ~ ., df) %>%
    step_sqrt(value) %>%
    step_center(value) %>%
    step_scale(value) %>%
    prep()
```

```{r}
df_processed <- bake(rec_obj, df)
```

```{r}
df_processed
```



Next, let’s capture the center/scale history so we can invert the center and scaling after modeling. The square-root transformation can be inverted by squaring the inverted center/scale values.

```{r}
center_history <- rec_obj$steps[[2]]$means["value"]
scale_history  <- rec_obj$steps[[3]]$sds["value"]

```

```{r}
c("center" = center_history, "scale" = scale_history)
```


###¤ LSTM plan
We need a plan for building the LSTM. First, a couple PRO TIPS with LSTM’s:

**Tensor Format:**

* Predictors (X) must be a 3D Array with dimensions: `[samples, timesteps, features]`: The first dimension is the length of values, the second is the number of time steps (lags), and the third is the number of predictors (1 if univariate or n if multivariate)
* Outcomes/Targets (y) must be a 2D Array with dimensions: [samples, timesteps]: The first dimension is the length of values and the second is the number of time steps (lags)

**Training/Testing:**

* The training and testing length must be evenly divisible (e.g. training length / testing length must be a whole number)

**Batch Size:**

* The batch size is the number of training examples in one forward/backward pass of a RNN before a weight update
* The batch size must be evenly divisible into both the training an testing lengths (e.g. training length / batch size and testing length / batch size must both be whole numbers)

**Time Steps:**

* A time step is the number of lags included in the training/testing set
* For our example, our we use a single lag

**Epochs:**

* The epochs are the total number of forward/backward pass iterations

Typically more improves model performance unless overfitting occurs at which time the validation accuracy/loss will not improve
Taking this in, we can come up with a plan. We’ll select a prediction of window 120 months (10 years) or the length of our test set. The best correlation occurs at 125, but this is not evenly divisible by the forecasting range. We could increase the forecast horizon, but this offers a minimal increase in autocorrelation. We can select a batch size of 40 units which evenly divides into the number of testing and training observations. We select `time steps = 1`, which is because we are only using one lag. Finally, we set `epochs = 10`, but this will need to be adjusted to balance the bias/variance tradeoff.

To keep track, lets upfront define the model inputs:

```{r}
lag_setting  <- 120 # = nrow(df_test)
batch_size   <- 40
train_length <- 440
tsteps       <- 1
epochs       <- 50
```

#### 2D and 3D TTrain/Test Arrays

We can then setup the training and testing sets in the correct format (arrays) as follows. Remember, LSTM’s need 3D arrays for predictors (X) and 2D arrays for outcomes/targets (y).

```{r}
# Training Set
lag_train_tbl <- df_processed %>%
    mutate(value_lag = lag(value, n = lag_setting)) %>%
    filter(!is.na(value_lag)) %>%
    filter(key == "training") %>%
    tail(train_length)
```

```{r}
x_train_vec <- lag_train_tbl$value_lag
x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
```

```{r}
y_train_vec <- lag_train_tbl$value
y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
```

```{r}
# Testing Set
lag_test_tbl <- df_processed %>%
    mutate(
        value_lag = lag(value, n = lag_setting)
    ) %>%
    filter(!is.na(value_lag)) %>%
    filter(key == "testing")
```

```{r}
x_test_vec <- lag_test_tbl$value_lag
x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
```

```{r}
y_test_vec <- lag_test_tbl$value
y_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))
```



#### Building the LSTM model

We’ll use two LSTM layers each with 50 units. 

1. The first LSTM layer takes the required input shape, which is the `[time steps, number of features]`. The batch size is just our batch size. We set the first layer to `return_sequences = TRUE` and `stateful = TRUE`. 
2. The second layer is the same with the exception of `batch_size`, which only needs to be specified in the first layer, and `return_sequences = FALSE` which does not return the time stamp dimension (2D shape is returned versus a 3D shape from the first LSTM). We here also add a bit of `dropout` and `recurrent_dropout` to fight overfitting.
3. We tack on a `layer_dense(units = 1),` which is the standard ending to a `keras` sequential model.



```{r}
model <- keras_model_sequential() %>%
  # # First LSTM Layer
    layer_lstm(units            = 50, 
               input_shape      = c(tsteps, 1), 
               batch_size       = batch_size,
               return_sequences = TRUE, 
               stateful         = TRUE) %>% 
  # Second LSTM Layer
    layer_lstm(units            = 50, 
               return_sequences = FALSE, 
               dropout	        = 0.1,
               recurrent_dropout= 0.1,
               stateful         = TRUE) %>% 
  # Final prediction layer
    layer_dense(units = 1)
```

Last, we `compile()` using the `loss = "mae"` (Mean Absolute Error) and the popular `optimizer = "rmsprop"`.

```{r}
model %>% 
    compile(loss = "mae", 
            optimizer = "rmsprop")
```

```{r}
model %>% summary()
```


# Fitting the model

Next, we can fit our stateful LSTM using a for loop (we do this to manually reset states). We set `shuffle = FALSE` to preserve sequences, and we manually reset the states after each epoch using `reset_states()`.

```{r}
for (i in 1:epochs) {
    model %>% fit(x          = x_train_arr, 
                  y          = y_train_arr, 
                  batch_size = batch_size,
                  epochs     = 1, 
                  verbose    = 1, 
                  shuffle    = FALSE)
    
    model %>% reset_states()
    cat("Epoch: ", i)
}
```

#### Predicting one Slice

We can then make predictions on the test set, `x_test_arr`, using the `predict()` function. We can retransform our predictions using the `scale_history` and `center_history`, which were previously saved and then squaring the result. Finally, we combine the predictions with the original data in one column using `reduce()` and a custom `time_bind_rows()` function.

```{r}
# Make Predictions
pred_out <- model %>% 
    predict(x_test_arr, batch_size = batch_size) %>%
    .[,1] 
```

```{r}
# Retransform values
pred_tbl <- tibble(
  index   = lag_test_tbl$index,
  value   = (pred_out * scale_history + center_history)^2
)
    
```

```{r}
# Combine actual data with predictions
tbl_1 <- df_train %>%
    mutate(key = "actual")

tbl_2 <- df_test %>%
    mutate(key = "actual")

tbl_3 <- pred_tbl %>%
    mutate(key = "predict")
```



```{r}
ret <- rbind.data.frame(tbl_1, tbl_2, tbl_3) %>%
  as_tibble() %>%
  arrange(key, index) %>%
  mutate(key = as_factor(key))
```


#### Assessing the performance on one split

We can use the `yardstick` package to assess performance using the `rmse()` function, which returns the root mean squared error (RMSE). Our data is in the long format (optimal format for visualizing with `ggplot2`), so we’ll create a wrapper function `calc_rmse()` that processes the data into the format needed for `yardstick::rmse()`.

```{r}
calc_rmse <- function(prediction_tbl) {
    
    rmse_calculation <- function(data) {
        data %>%
            spread(key = key, value = value) %>%
            select(-index) %>%
            filter(!is.na(predict)) %>%
            rename(
                truth    = actual,
                estimate = predict
            ) %>%
            rmse(truth, estimate)
    }
    
    safe_rmse <- possibly(rmse_calculation, otherwise = NA)
    
    safe_rmse(prediction_tbl)
        
}
```


We can inspect the RMSE on the model.

```{r}
calc_rmse(ret)
```

The RMSE doesn’t tell us the story. We need to visualize. Note - The RMSE will come in handy in determining an expected error when we scale to all samples in the backtesting strategy.

# Visualization

Next, we make a plotting function, `plot_prediction()`, using `ggplot2` to visualize the results for a single sample.

```{r}
plot_prediction <- function(data, id, alpha = 1, size = 2, base_size = 14) {
  
    rmse_val <- calc_rmse(data)
    
    g <- data %>%
        ggplot(aes(index, value, color = key)) +
        geom_point(alpha = alpha, size = size) + 
        tidyquant::theme_tq(base_size = base_size) +
        tidyquant::scale_color_tq() +
        labs(title = paste(id, "RMSE:", round(rmse_val$.estimate, digits = 1), sep = " "),
             x = "", y = ""
        )
    
    return(g)
}
```

We can test out the plotting function setting the `id = split_id`, which is “Slice9”.

```{r}
ret %>% 
    plot_prediction(id = split_id, alpha = 0.65) +
    theme(legend.position = "bottom")
```

Doesnt look that bad, right? :) The LSTM performed relatively well! The settings we selected seem to produce a decent model that picked up the trend. It jumped the gun on the next up-trend, but overall it exceeded my expectations. Now, we need to Backtest to see the true performance over time!

## LSTM on the whole Sample including BAcktesting

Once we have the LSTM working for one sample, scaling to all 11 is relatively simple. We just need to create an prediction function that can be mapped to the sampling plan data contained in `rolling_origin_resamples`.

### Creating the Mapping Function

This step looks intimidating, but it’s actually straightforward. We copy the code from theingle LSTM into a function. We’ll make it a safe function, which is a good practice for any long-running mapping processes to prevent a single failure from stopping the entire process.

```{r}
predict_keras_lstm <- function(split, epochs = 10, ...) {
    
    lstm_prediction <- function(split, epochs, ...) {
        
        # Data Setup
        df_train <- training(split)
        df_test <- testing(split)
        
        df <- rbind(
            df_train %>% mutate(key = "training"),
            df_test %>% mutate(key = "testing"))
        
        # Preprocessing
        rec_obj <- recipe(value ~ ., df) %>%
            step_sqrt(value) %>%
            step_center(value) %>%
            step_scale(value) %>%
            prep()
        
        df_processed_tbl <- bake(rec_obj, df)
        
        center_history <- rec_obj$steps[[2]]$means["value"]
        scale_history  <- rec_obj$steps[[3]]$sds["value"]
        
        # LSTM Plan
        lag_setting  <- lag_setting # = nrow(df_tst)
        batch_size   <- batch_size
        train_length <- train_length
        tsteps       <- tsteps
        epochs       <- epochs
        
        # Train/Test Setup
        lag_train_tbl <- df_processed_tbl %>%
            mutate(value_lag = lag(value, n = lag_setting)) %>%
            filter(!is.na(value_lag)) %>%
            filter(key == "training") %>%
            tail(train_length)
        
        x_train_vec <- lag_train_tbl$value_lag
        x_train_arr <- array(data = x_train_vec, dim = c(length(x_train_vec), 1, 1))
        
        y_train_vec <- lag_train_tbl$value
        y_train_arr <- array(data = y_train_vec, dim = c(length(y_train_vec), 1))
        
        lag_test_tbl <- df_processed_tbl %>%
            mutate(value_lag = lag(value, n = lag_setting)) %>%
            filter(!is.na(value_lag)) %>%
            filter(key == "testing")
        
        x_test_vec <- lag_test_tbl$value_lag
        x_test_arr <- array(data = x_test_vec, dim = c(length(x_test_vec), 1, 1))
        
        y_test_vec <- lag_test_tbl$value
        y_test_arr <- array(data = y_test_vec, dim = c(length(y_test_vec), 1))
                
        # LSTM Model
        model <- keras_model_sequential() %>%
          # # First LSTM Layer
          layer_lstm(units      = 50, 
               input_shape      = c(tsteps, 1), 
               batch_size       = batch_size,
               return_sequences = TRUE, 
               stateful         = TRUE) %>% 
          # Second LSTM Layer
          layer_lstm(units      = 50, 
               return_sequences = FALSE, 
               dropout	        = 0.1,
               recurrent_dropout= 0.1,
               stateful         = TRUE) %>% 
          # Final prediction layer
          layer_dense(units = 1)
        
        model %>% 
            compile(loss = 'mae', optimizer = 'adam')
        
        # Fitting LSTM
        for (i in 1:epochs) {
            model %>% fit(x          = x_train_arr, 
                          y          = y_train_arr, 
                          batch_size = batch_size,
                          epochs     = 1, 
                          verbose    = 1, 
                          shuffle    = FALSE)
            
            model %>% reset_states()
            cat("Epoch: ", i)
            
        }
        
        # Predict and Return Tidy Data
        # Make Predictions
        pred_out <- model %>% 
            predict(x_test_arr, batch_size = batch_size) %>%
            .[,1] 
        
        # Retransform values
        pred_tbl <- tibble(
            index   = lag_test_tbl$index,
            value   = (pred_out * scale_history + center_history)^2
        ) 
        
        # Combine actual data with predictions
        tbl_1 <- df_train %>%
            mutate(key = "actual")
        
        tbl_2 <- df_test %>%
            mutate(key = "actual")
        
        tbl_3 <- pred_tbl %>%
            mutate(key = "predict")
        
        
        ret <- rbind.data.frame(tbl_1, tbl_2, tbl_3) %>%
          as_tibble() %>%
          arrange(key, index) %>%
          mutate(key = as_factor(key))

        return(ret)
        
    }
    
    safe_lstm <- possibly(lstm_prediction, otherwise = NA)
    
    safe_lstm(split, epochs, ...)
    
}
```


We can test the custom `predict_keras_lstm()` function out with 10 epochs. It returns the data in long format with `actual` and `predict` values in the key column.

```{r}
predict_keras_lstm(split, epochs = 10)
```

### Mapping the LSTM prediction over all samples

With the `predict_keras_lstm()` function in hand that works on one split, we can now map to all samples using a `mutate()` and `map()` combo. The predictions will be stored in a `list` column called `predict`. Note that this will take 5-10 minutes or so to complete.

```{r}
sample_predictions_lstm_tbl <- rolling_origin_resamples %>%
     mutate(predict = map(splits, predict_keras_lstm, epochs = 10))
```

We now have the predictions in the column “predict” for all  splits!.

```{r}
sample_predictions_lstm_tbl
```


### Prediction Performance

We can assess the RMSE by mapping the `calc_rmse()` function to the “predict” column.

```{r}
sample_rmse_tbl <- sample_predictions_lstm_tbl %>%
    mutate(rmse = map_df(predict, calc_rmse) %>% pull(.estimate)) %>%
    select(id, rmse)

sample_rmse_tbl
```

We can also plot it

```{r}
sample_rmse_tbl %>%
    ggplot(aes(rmse)) +
    geom_histogram(aes(y = ..density..), fill = tidyquant::palette_light()[[1]], bins = 16) +
    geom_density(fill = tidyquant::palette_light()[[1]], alpha = 0.5) +
    tidyquant::theme_tq() +
    ggtitle("Histogram of RMSE")
```

And, we can summarize the RMSE for the 11 slices. PRO TIP: Using the average and standard deviation of the RMSE (or other similar metric) is a good way to compare the performance of various models.

```{r}
sample_rmse_tbl %>%
    summarize(
        mean_rmse = mean(rmse),
        sd_rmse   = sd(rmse)
    )
```

### Visualization

We can create a `plot_predictions()` function that returns one plot with the predictions for the entire set of 11 backtesting samples!!!

```{r}
plot_predictions <- function(sampling_tbl, predictions_col, 
                             ncol = 3, alpha = 1, size = 2, base_size = 14,
                             title = "Backtested Predictions") {
    
  require(ggpubr)
  require(cowplot)
  predictions_col_expr <- enquo(predictions_col)
    
  # Map plot_split() to sampling_tbl
  sampling_tbl_with_plots <- sampling_tbl %>%
    mutate(gg_plots = map2(!! predictions_col_expr, id, 
                               .f        = plot_prediction, 
                               alpha     = alpha, 
                               size      = size, 
                               base_size = base_size)) 
    
  # Make plots with cowplot
  plot_list <- sampling_tbl_with_plots$gg_plots 
  p_temp <- plot_list[[1]] + theme(legend.position = "bottom")
  legend <- get_legend(p_temp)
  
  p_body  <- plot_grid(plotlist = plot_list, ncol = ncol)
  
  p_title <- ggdraw() + 
    draw_label(title, size = 18, fontface = "bold", colour = palette_light()[[1]])
    
  g <- plot_grid(p_title, p_body, legend, ncol = 1, rel_heights = c(0.05, 1, 0.05))
  
  return(g)
}
```

Here’s the result. On a data set that’s not easy to predict, this is quite impressive!!

```{r, fig.height=12.5,fig.width=12.5}
sample_predictions_lstm_tbl %>%
    plot_predictions(predictions_col = predict, alpha = 0.5, size = 1, base_size = 10,
                     title = "Keras Stateful LSTM: Backtested Predictions")
```

# Endnotes

### References

### More info


### Session info
```{r}
sessionInfo()
```