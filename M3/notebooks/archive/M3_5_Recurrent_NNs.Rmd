---
title: 'M3-5: Recurrent Neural Networks (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    code_folding: hide
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation
set.seed(1337) # To have a seed defined for reproducability

### Knitr options
if (!require("knitr")) install.packages("knitr"); library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     fig.align="center"
                     )

### Install packages if necessary
if (!require("pacman")) install.packages("pacman") # package for loading and checking packages :)
pacman::p_load(tidyverse, # Standard datasciewnce toolkid (dplyr, ggplot2 et al.)
               magrittr # For advanced piping (%>% et al.)
               )
```

```{r}
### Load packages  Standard
library(caret) # for train / control ML workflows
library(keras) # For deep learning
# Sys.setenv(RETICULATE_PYTHON="/usr/local/share/.virtualenvs/r-reticulate/bin/") # Only necessary in kaggle currently
```



# Introduction to time series and sequential approaches to neural Neutworks

A major characteristic of all neural networks you haveve seen so far, such as **densely connected** networks and **convnets,** is that they have no memory. Each input shown to them is processed independently, with no state kept in between inputs. This is fine for cross-sectional prediction problems such as identifying cats and dogs on pictures. However, many interesting real-life problems such as predicting stock prices, biofunctions, or the next word in a text are based on time-series or other types of sequential data. Here, the ability to incorporate information on past states of the outcome and/or other features often greatly enhances the performance of our predictions, since these past states carry relevant information for future development.

## Sequential prediction problems

Using neural networks for such sequential problems, in order to process a sequence or a temporal series of data points, you have to show the entire sequence to the network at once: turn it into a single data point. Therefore, often we would like to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:

![](https://www.dropbox.com/s/92m31bwn1dtvu88/DL_lstm_1.jpeg?dl=1){width=750px}

Sounds way more interesting, right? So. how do we do that? Lets find out!


## Working with sequential data

Within human reasoning, we utulize information on past states naturally all the time. As you're reading (or listening) the present sentence, you’re processing it word by word—or rather, eye saccade by eye saccade—while keeping memories of what came before; this gives you a fluid representation of the meaning conveyed by this sentence. Biological intelligence processes information incrementally while maintaining an internal model of what it’s processing, built from past information and constantly updated as new information comes in. Our thoughts have persistence.

Traditional neural networks can’t do this, and it seems like a major shortcoming. For example, imagine you want to classify what kind of event is happening at every point in a movie. Or a moving stock on the market, predicted just with a snapshot of its indicators at one point in time. It’s unclear how a traditional neural network could use its reasoning about previous events to inform later ones. 

**Recurrent neural networks** (RNN) address this issue in a very simplified way. They are networks with loops in them, allowing information to persist.

MOre precisely, it processes sequences by iterating through the sequence elements and maintaining a **state** containing information relative to what it has seen so far. In effect, an RNN is a type of neural network that has an internal loop The state of the RNN is reset between processing two different, independent sequences, so you still consider one sequence a single data point: a single input to the network. What changes is that this data point is no longer processed in a single step; rather, the network internally loops over sequence elements.

## Loops

To make these notions of **loop** and **state** clear, let’s implement the **forward pass** of a toy RNN in `R`. This RNN takes as input a sequence of vectors, which you’ll encode as a 2D tensor of size (`timesteps`, `input_features`). It loops over timesteps, and at each timestep, it considers its current state at `t` and the input at `t` (of shape (`input_features`), and combines them to obtain the output at `t`. You’ll then set the state for the next step to be this previous output. For the first timestep, the previous output isn’t defined; hence, there is no current state. So, you’ll initialize the state as an all-zero vector called the initial state of the network.

In pseudocode, this is the RNN.

```{r,eval=FALSE}
state_t = 0    
# the state as t
for (input_t in input_sequence) {        # Iterates over sequence elements
  output_t <- f(input_t, state_t)
  state_t <- output_t                    # The previous output becomes the state for the next iteration.
}
```

You can even flesh out the function `f`: the transformation of the input and state into an output will be parameterized by two matrices, `W` and `U`, and a bias vector `b`. It’s similar to the transformation operated by a densely connected layer in a feedforward network, right?

```{r,eval=FALSE}
state_t <- 0

for (input_t in input_sequence) {
  output_t <- activation(dot(W, input_t) + dot(U, state_t) + b)
  state_t <- output_t
}
```

To make these notions absolutely unambiguous, let’s write a naive `R` implementation of the forward pass of the simple RNN.

```{r}
# We define a function that produces a "random" array
random_array <- function(dim) {
  array(runif(prod(dim)), dim = dim)
}
```

```{r}
# We define some constants as arguments for the functions to come
timesteps <- 100 # Number of timesteps in the input sequence
input_features <- 32 # Dimensionality of the input feature space
output_features <- 64 # Dimensionality of the output feature space
```

```{r}
# We create the initial inputs
inputs <- random_array(dim = c(timesteps, input_features)) # Input data: random noise for the sake of the example
state_t <- rep_len(0, length = c(output_features)) # Initial state t: an all-zero vector
```

```{r}
# We create some random matrices for W, U, b
W <- random_array(dim = c(output_features, input_features)) # Creates random weight matrices: W
U <- random_array(dim = c(output_features, output_features)) # Creates random weight matrices: U
b <- random_array(dim = c(output_features, 1)) # Creates random weight matrices: b
```

```{r}
# We create an empty array for the output sequence
output_sequence <- array(0, dim = c(timesteps, output_features))
```

```{r}
dim(output_sequence)
```

```{r}
head(output_sequence[,1:5])
```


So, we just made up an random `inputs` array, lets take a little look:

```{r}
dim(inputs)
```

```{r}
head(inputs[,1:5])
```

There we go. We created an 2d tensor, where we have an vector of 32 features over 100 timesteps. Likewise, we created a random weight matrix `W` (weights for `t`) and `U` (weights for `t-1`) of dimensionality 64x32 (we want 64 outputs), and a bias vector of lenght 64.

```{r}
dim(W)
```

```{r}
head(W[,1:5])
```


All set up, lets run a loop, where we apply some activation function (here `tanh()`) on the weighted `input_t`, but we add the (in another way) weighted `state_t` (the lagged value of `input_t` $\rightarrow$ `input_t-1`).

**Note:**  I use `tanh()` just as an example for whatever activation function you might want to apply to your inputs. `tanh()` (hyperbolic  tangent) is popular for RNNs, since it squishes input values between a range of `[-1,1]`

```{r}
for (i in 1:nrow(inputs)) {
  input_t <- inputs[i,]                                                # input_t is a vector of shape (input_features)
  output_t <- tanh(as.numeric((W %*% input_t) + (U %*% state_t) + b))  # Combines the input with the current state (the previous output) 
  output_sequence[i,] <- as.numeric(output_t)                          # Updates the result matrix
  state_t <- output_t                                                  # Updates the state of the network for the next timestep
}
```

```{r}
glimpse(output_sequence)
```


**Note:**  In `U %*% state_t`, the `%*%` operator performs a real matrix multiplication (every element of `U` gets multiplied with every element of `state_t`), not the dotproduct (cell-wise multiplication), as `U * state_t` would.

Easy enough: in summary, an RNN is an neural network application of a `for()` (reuses values computed during the previous iteration of the loop), nothing more. Of course, there are many different RNNs fitting this definition that you could build—this example is one of the simplest RNN formulations. Anyhow, In case we would not have to train the weights, we are done by here.

RNNs are characterized by their step function, such as the following function in this case

```{r}
output_t <- tanh(as.numeric((W %*% input_t) + (U %*% state_t) + b))
```

```{r}
glimpse(output_t)
```


So, that's pretty much how to construct a recurrent layer "by hand". Most probably you will never have to, but I like to believe it demystifies the whole process.Moving on...

## Recurrent Layers in `Keras`

The process you just naively implemented in `R` corresponds to an actual `Keras` layer: `layer_simple_rnn`

```{r}
layer_simple_rnn(units = 32)
```

Like all recurrent layers in `Keras`, `layer_simple_rnn` can be run in two different modes: 

1. it can return either the full sequences of successive outputs for each timestep (a 3D tensor of shape `(batch_size, timesteps, output_features)`) 
2. or only the last output for each input sequence (a 2D tensor of shape `(batch_size, output_features)`). 

These two modes are controlled by the `return_sequences` constructor argument. Let’s look at an example that uses `layer_simple_rnn` and returns only the output at the last timestep:

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>% # About this type of layer, we talk later
  layer_simple_rnn(units = 32)
```

```{r}
summary(model)
```


The following example in turn returns the full state sequence (`return_sequences = TRUE`):

```{r}
model_sequence <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE)
```

```{r}
summary(model_sequence)
```

It’s sometimes useful to stack several recurrent layers one after the other in order to increase the representational power of a network. In such a setup, you have to get all of the intermediate layers to return full sequences. More on that later.

```{r}
model_sequence_stacked <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, return_sequences = TRUE) %>%
  layer_simple_rnn(units = 32, return_sequences = FALSE)
```

```{r}
summary(model_sequence_stacked)
```

```{r,include=FALSE}
rm(list=ls())
```

# Recurrent Neural Networks (a text example)

So, lets start applying a first RNN, and since you are already warmed up, we do it with text data. It is not hard to see why for making sense of text data, sequential models would be a good idea. 

## The IMDB dataset

You’ll work with the IMDB dataset: a set of 50,000 highly polarized movie reviews from the Internet Movie Database, labeled by sentiment (positive/negative). They’re split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting of 50% negative and 50% positive reviews. Read more details [here](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification). in case you are interested.

Just like the MNIST dataset, the IMDB dataset comes packaged with `Keras`, and is already neathly preprocessed. Each review is encoded as a **sequence** of word indexes (integers), where each integer stands for a specific word in a dictionary. For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer `3` encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: "only consider the top `10,000` most common words, but eliminate the top `20` most common words" (Note: `0` is the encoding for unknown words). 

### Load the data

```{r}
library(keras)
imdb <- dataset_imdb(num_words = 10000)
```

```{r}
#c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```


**Note:** The datasets built into Keras are all nested lists of training and test data. Here, we use the multi-assignment operator `%<-%` from the `zeallot` package to unpack the list into a set of distinct variables. This is only a convenience function, and could equally be written as follows:

```{r}
train_data <- imdb$train$x
train_labels <- imdb$train$y

test_data <- imdb$test$x
test_labels <- imdb$test$y
```

The argument `num_words = 10000` means we keep only the top 10,000 most frequently occurring words in the training data. Rare words will be discarded. This allows you to work with vector data of a manageable size.

The variables `train_data` and `test_data` are lists of reviews; each review is a list of word indices (encoding a sequence of words). `train_labels` and `test_labels` are lists of 0s and 1s, where `0` stands for negative and `1` stands for positive:

```{r}
glimpse(train_labels[1:5])
```

```{r}
glimpse(train_data[1:5])
```


Because you’re restricting yourself to the top 10,000 most frequent words, no word index will exceed 10,000:

```{r}
train_data %>% map_int(max) %>% max()
```


Sidenote: Notice that all words are not appearing anymore as strings, but as a numeric index. This is very pratical to do as preprocesing of text data before using them for ML workflows. However, we cannot read the reviews anymore manyally. Only for fun, here’s how you can quickly decode one of these reviews back to English words:

```{r}
word_index <- dataset_imdb_word_index() # word_index is a named list mapping words to an integer index.
```

```{r}
word_index %>% head()
```

```{r}
# lets do it the other way around
reverse_word_index <- names(word_index)
names(reverse_word_index) <- word_index
```

```{r}
reverse_word_index %>% head()
```


```{r}
# Reverses it, mapping integer indices to word
decoded_review <- sapply(train_data[[1]], function(index) {                # Decodes the review
  word <- if (index >= 3) reverse_word_index[[as.character(index - 3)]]
  if (!is.null(word)) word else "?"
})
# Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for “padding,” “start of sequence,” and “unknown.”
```

```{r}
decoded_review
```


```{r,include=FALSE}
rm(word_index, reverse_word_index, decoded_review)
```


### Preprocessing

So, we have our data ready. However, we cannot can’t feed lists of integers (keep in mind, they are supposed to not represent numbers but words) into a neural network. You have to turn your lists into tensors. There are two ways to do that:

1. **One-hot encode** your lists to turn them into vectors of 0s and 1s. This would mean, for instance, turning the sequence `[3, 5]` into a 10,000-dimensional vector that would be all 0s except for indices `3` and `5`, which would be `1`. Then you could use as the first layer in your network a dense layer, capable of handling floating-point vector data.
2. **Pad** your lists so that they all have the same length, turn them into an integer tensor of shape `(samples, word_indices)`, and then use as the first layer in your network a layer capable of handling such integer tensors (the “embedding” layer, comming soon).

While the first approach sounds computationally very inefficient, it is also the most intuitive to operationalize in terms of data-munging.  Let’s warm up with this first solution and vectorize the data, which we will do manually for maximum clarity.

```{r}
# Again, a small function that creates a 0-matrix, and replaces the corresponding words with 1.
vectorize_sequences <- function(sequences, dimension) {
  results <- matrix(0, nrow = length(sequences), ncol = dimension) # Creates an all-zero matrix of shape (length(sequences), dimension)
  for(i in 1:length(sequences)){
    results[i, sequences[[i]]] <- 1 # Sets specific indices of results[i] to 1s
  }
  return(results)
}
```

```{r}
x_train <- train_data %>% vectorize_sequences(dimension = 10000) 
x_test <- test_data %>% vectorize_sequences(dimension = 10000) 
```


Here’s what the samples look like now:

```{r}
str(x_train[1,])
```

**Hint:** The `keras` function `to_categorical()`, which does exactly the same more convenient when the data is in a dataframe shape.

We now only recode the outcomes as numerical.

```{r}
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
```

Now the data is ready to be fed into a neural network.

**Note:** This was only 

## A baseline feed-forward ANN

So far so good, let's run a "normal" feed-forward ANN to predict the review sentiment. We do so, just to warm up and get a sense how good it performs. A little reminder, to build and run a `Keras` model, you need to:

1: Define the architecture (Layers, shape & activation)
2: Compile the model (choosing optimizer, loss function, and metric)
3: Run the model

### MOdel architecture

We use, like before, a simple feed forward architecture with an input layer of shape 10.000 (number of words in our review vector), 2 inetrmediate hidden layers with 16 units each, and an ouput layer with only 1 unit (since we perform a binary classification). For the hidden layers, we use the standard `relu` activation function, for the output we use `sigmoid`, as common for binary classification. Note that all architecture choices in this example are standard, but not necessarily informed by our data. Just a good rule-of-thumb starting point.

```{r}
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(10000)) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Finally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the `binary_crossentropy` loss. It isn’t the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions. For the optimizer we go for the allrounder `rmsprop`, since we in this case have no reason to believe otherwise.

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

So, we are pretty much done. Due to the big fiorst layer, we have a lot of parameter, but otherwise nothing extraordinary happening yet.

```{r}
summary(model)
```

Lets run it!

```{r}
history <- model %>% fit(
  x_train,
  y_train,
  epochs = 10,
  batch_size = 512,
  validation_split = 0.25
)
```

And lets take a look how the accuracy and loss developed over the epocs.

```{r}
plot(history)
```

Ok, agaion we see that we are overfitting. While accuracy and loss as in most cases further increases during the epocs, we see after 3 epocs the metrics to decline on our validation data. At one point we have to deal with that, but not now.

Lets just sum up: 

* We created a one-hot-encoding term-document matrix for the most 10.000 frequent terms used in Imdb reviews, and used that to predict if the review had a positive or negative sentiment.
* We did so by feeding this 10.000 dimensional vector as a 2d tensor of shape `(sample, features)` to a simple feed-forward network with pretty standard architecture
* We got an accuracy somewhere between 85-90% in the validation sample.

I would say, not too bad at all. However, represenmting a review as a 10.000 dimensional vector of one-hot encodings of word occurence appears pretty blunt. There must be something better, right? 


## A Recurrent Neural Network Approach to text data in `Keras`

So, now that we tried a pretty naive model as baseline, lets move on.


### Preprocessing

In our first baseline model, we used a document-term matrix as inputs for training, with one-hot-encodings (= dummy variables) for the 10.000 most popular terms. This has a couple of disadvantages. Besides being a very large and sparse vector for every review, as a "bag-of-words", it did not take the word-order (sequence) into account. 

This time, we use a different approach, therefore also need a different input data-structure. We now use `pad_sequences()` to create a integer tensor of shape `(samples, word_indices)`. However, reviews vary in lenght, which is a problem since `Keras` reqieres the inputs to have the same shape across the whole sample. Therefore, we use the `maxlen = 500` argument, to restrict ourselves to the first 500 words in every review.

As a consequence, longer reviews will be cut at 500 words, and shorter reviews will be filled up with 0 values.

```{r}
x_train <- pad_sequences(train_data, maxlen = 500)
x_test <- pad_sequences(test_data, maxlen = 500)
```

Lets take a look:

```{r}
glimpse(x_train)
```

We see that we indeed end up with word sequences. However, also notice that we have quite a bunch of `0`s, representing unknown words.

Lets set up our model. as discussed, we will first use a `layer_embedding` to compress our initial one-hot-encoding vector of lenght 10.000 to a "meaning-vector" (=embedding) of the lower dimensionality of 32. We did not talk about that too much, but the next session will dig deeper into that. Just take it for now...

Then we add a `layer_simple_rnn` on top, and finally a `layer_dense` for the binary prediction of review sentiment.

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
  layer_simple_rnn(units = 32, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")
```

```{r}
summary(model)
```

THe further parameters are quite conventional and by now well-known.

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

And we already run the model:

```{r}
history <- model %>% fit(
  x_train, y_train,
  epochs = 10,
  batch_size = 128,
  validation_split = 0.2
)
```

Yes, I know, this one takes a while...

In adittion, the model performs quite dissapointing. After about 4 periods, it starts overfitting, as often. However, the highest validation accuracy we get during the epochs is slightly higher than 85%, which is no improvement compared to our baseling. Part of the problem is that your inputs only consider the first 500 words, rather than full sequences—hence, the RNN has access to less information than the earlier baseline model. 

The insight of the problem is that `layer_simple_rnn` isn’t good at processing long sequences, such as 500+ words review text. The reason is that words often derive their meaning not only from the one word before, but a longer sequence, eg. the whole sentence in which they occur. For such settings, other types of recurrent layers perform much better. Let’s go on exploring.


# Long Short Term Memory networks

One of the appeals of RNNs is the idea that they might be able to connect previous information to the present task, such as using previous word sequences might inform the understanding of the present word in context. If RNNs could do this, they’d be extremely useful. We already saw in the example before, that going one step back is often not good enough, since meaning in some cases unfolds over a long sequence. 

So, could we just stack several `layer_simple_rnn`on top of each other to capture the information contained in long sequences? Kind of, but there is a probhlem...

In theory, RNNs are absolutely capable of handling such “long-term dependencies.” A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don’t seem to be able to learn them. The problem was explored in depth by Hochreiter (1991, in German) and Bengio, et al. (1994)[^1], who found some pretty fundamental reasons why it might be difficult. First and foremost, the **valishing gradient** problem.

## Backpropagation Through Time (BPTT) and the Vanishing (Exploding) Gradient
Remember, the purpose of RNNs is to accurately classify sequential input. We rely on the backpropagation of error and gradient descent to do so. RNNs rely on an extension of backpropagation called **backpropagation through time**, or BPTT. Time, in this case, is simply expressed by a well-defined, ordered series of calculations linking one time step to the next, which is all backpropagation needs to work. Remember, neural networks, whether they are recurrent or not, are simply nested composite functions like `f(g(h(x)))`. Adding a time element only extends the series of functions for which we calculate derivatives with the chain rule.

This is partially because the information flowing through neural nets passes through many stages of multiplication.  Because the layers and time steps of deep neural networks relate to each other through multiplication, derivatives are susceptible to vanishing or exploding. This is an effect that is similar to what is observed with non-recurrent networks (feedforward networks) that are many layers deep: as you keep adding layers to a network, the network eventually becomes untrainable.

Exploding gradients treat every weight as though it were the proverbial butterfly whose flapping wings cause a distant hurricane. Those weights’ gradients become saturated on the high end; i.e. they are presumed to be too powerful. But exploding gradients can be solved relatively easily, because they can be truncated or squashed. Vanishing gradients can become too small for computers to work with or for networks to learn – a harder problem to solve.

Below you see the effects of applying a sigmoid function over and over again. The data is flattened until, for large stretches, it has no detectable slope. This is analogous to a gradient vanishing as it passes through many layers.

![](https://www.dropbox.com/s/a8re7503p7ehhv5/DL_vanishing_gradient.png?dl=1){width=750px}

## Long Short-Term Memory Units (LSTMs)

### Introduction

Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were  were introduced by Hochreiter & Schmidhuber (1997)[^2], and represent the cunnulation of their work on the vanishing gradient problem. They work tremendously well on a large variety of problems, and are now widely used.

After all, their idea was simple: They introduced a variant of `layer_simple_rnn`; it adds a way to carry information across many timesteps. Imagine a conveyor belt running parallel to the sequence you’re processing. Information from the sequence can jump onto the conveyor belt at any point, be transported to a later timestep, and jump off, intact, when you need it. 

This is essentially what LSTM does: it saves information for later, thus preventing older signals from gradually vanishing during processing. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!

All recurrent neural networks have the form of a chain of repeating modules of neural network. In standard RNNs, this repeating module will have a very simple structure, such as a single `tanh()` layer. It takes (differently) weighted input from the cells in the layer below, as well as of the **state**, which is just the "saved"/"remembered" layer below how it was one time-step earlier (at t-1). THen, with the `tanh()`, it squisches all the weights together to an output bounded between `[-1,1]`.

![](https://www.dropbox.com/s/uf2tzn7zo5wiwwv/DL_LSTM1.png?dl=1){width=750px}

### The structure of LSTMs

LSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way. In adittion to the layer with the `tanh()` activation function that produces `[-1,1]` outputs, we see adittional `sigmoid()` layers that produce output between `[0,1]`

![](https://www.dropbox.com/s/3d8v9xltho8fm8u/DL_LSTM2.png?dl=2){width=750px}

Don’t worry about the details of what’s going on. We will do that one step by step. Lets rget first comfortable with the notation:

![](https://www.dropbox.com/s/1x1szsp7jnhlg5l/DL_LSTM_notation.png?dl=1){width=750px}

In the above diagram, each line carries an entire vector, from the output of one node to the inputs of others. 

* Pink circles represent pointwise operations, like vector addition
* Yellow boxes are learned neural network layers. 
* Lines merging denote concatenation
* Line forking denote its content being copied and the copies going to different locations.


![](https://www.dropbox.com/s/b0etqeu9vjzvw05/DL_LSTM3.png?dl=1){width=750px}

The key to LSTMs is the cell **state**, the horizontal line running through the top of the diagram. The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.

The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called **gates**. Gates are a way to optionally let information through. They are composed out of a sigmoid neural net layer and a pointwise multiplication operation. 

The sigmoid layer outputs numbers between zero and one, describing how much of each component should be let through. A value of zero means “let nothing through,” while a value of one means “let everything through!” An LSTM has three of these gates, to protect and control the cell state.

1. The Forget Gate
2. The Input gate
3. The Output Gate

Conceptually, that is how an LSTM learns to remember as well as to forget. Lets inspect these layers more in detail.


#### 1: The Forget Gate Layer

The first step in our LSTM is to decide what information we’re going to throw away from the cell state. This decision is made by a `sigmoid` layer called the **forget gate layer.*”**  A `1` represents “completely keep this” while a `0` represents “completely get rid of this.”

Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.

![](https://www.dropbox.com/s/opgkh6j3a6l2qcg/DL_LSTM_G1.png?dl=1){width=750px}


#### 2: The Input Gate Layer

The next step is to decide what new information we’re going to store in the cell state. This has two parts. First, a `sigmoid` layer called the **input gate layer** decides which values we’ll update. Next, a `tanh` layer creates a vector of new candidate values, `Ct`, that could be added to the state. In the next step, we’ll combine these two to create an update to the state.

In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.

![](https://www.dropbox.com/s/vw5dacnvk8egn2q/DL_LSTM_G2.png?dl=1){width=750px}


It’s now time to update the old cell state, `Ct−1`, into the new cell state `Ct`. The previous steps already decided what to do, we just need to actually do it. We multiply the old state by `ft`, forgetting the things we decided to forget earlier. Then we add `it∗Ct`. This is the new candidate values, scaled by how much we decided to update each state value.

In the case of the language model, this is where we’d actually drop the information about the old subject’s gender and add the new information, as we decided in the previous steps.

![](https://www.dropbox.com/s/h083xx9av02nzs6/DL_LSTM_G3.png?dl=1){width=750px}


#### 3: The Output Gate Layer

Finally, we need to decide what we’re going to output. This output will be based on our cell state, but will be a filtered version. First, we run a `sigmoid` layer which decides what parts of the cell state we’re going to output. Then, we put the cell state through `tanh` (to push the values to be between [−1,1]) and multiply it by the output of the `sigmoid` gate, so that we only output the parts we decided to.

For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.

![](https://www.dropbox.com/s/2dff1ek17kibp0u/DL_LSTM_G4.png?dl=1){width=750px}


#### Summary 

And, thats pretty much it. Sounds complicated, and to some extent it is. However, the concept is pretty straightforward. In order to exploit the information of multiple recurrent layers, we have to find a way to train the weights while avoiding the vanishing gradient problem. 

Therefore we introduce gates, through which previous states can pass through quasi unaltered. However, not every information from previous sequences is useful for us. Indeed, too much information could be a curse rather than a blessing for our model. Therefore, we have to introduce further gates that decide which information from previous stages to remember, and which to forget.

Inj `Keras` we can envision the architectur of an LSTM as follows:

![](https://www.dropbox.com/s/auxx96y43wn9unf/DL_LSTM_architecture.png?dl=1){width=750px}


### Application

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = 10000, output_dim = 32) %>%
  layer_lstm(units = 500, dropout = 0.25, recurrent_dropout = 0.25, return_sequences = FALSE) %>%
  layer_dense(units = 1, activation = "sigmoid")
```

Finally, you need to choose a loss function and an optimizer. Because you’re facing a binary classification problem and the output of your network is a probability (you end your network with a single-unit layer with a sigmoid activation), it’s best to use the `binary_crossentropy` loss. It isn’t the only viable choice: you could use, for instance, `mean_squared_error`. But crossentropy is usually the best choice when you’re dealing with models that output probabilities. Crossentropy is a quantity from the field of Information Theory that measures the distance between probability distributions or, in this case, between the ground-truth distribution and your predictions. For the optimizer we go for the allrounder `rmsprop`, since we in this case have no reason to believe otherwise.

```{r}
model %>% compile(
  optimizer = "rmsprop",
  loss = "binary_crossentropy",
  metrics = "accuracy"
)
```

```{r}
summary(model)
```

Lets run it!

```{r}
#history <- model %>% fit(
#  x_train,
#  y_train,
#  epochs = 5,
#  batch_size = 512,
#  validation_split = 0.25
#)
```

And lets take a look how the accuracy and loss developed over the epocs.

```{r}
#plot(history)
```

Well...


# Endnotes

### References

[^1] Yoshua Bengio, Patrice Simard, and Paolo Frasconi, “Learning Long-Term Dependencies with Gradient Descent Is Difficult,” IEEE Transactions on Neural Networks 5, no. 2 (1994).

[^2]Sepp Hochreiter and Jürgen Schmidhuber, “Long Short-Term Memory,” Neural Computation 9, no. 8 (1997).

### More info
You can find more info about:

* `keras` [here](https://keras.rstudio.com/)

### Session info
```{r}
sessionInfo()
```


