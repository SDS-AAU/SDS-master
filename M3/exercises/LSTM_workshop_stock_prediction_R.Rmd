---
title:  'Sequence-2-Sequence forecasting (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(magrittr)
library(keras)
library(tidymodels) # Or only load the 'rsample' and recipes on its own
```


# Workshop Stock prediction

Task:

1. Get some stock data (tip: Use tidyquant)
    * Limit yourself for now to on e stock
    * Limit yourself to one variable (preferably some price data)
2. Develop a one-step ahead prediction of prices (or their movements)

# Load some data

## Select a stock abnd load the data

* We will use the tidyquant package to download stock data

```{r}
library(tidyquant) # My favorite package to get stock data
```


```{r}
tickers = c("AAPL") # Just for fun, we get a couple of stocks
            
data_stocks <- tq_get(tickers,
               from = "2000-01-01",
               to = "2020-11-30",
               get = "stock.prices" # What we want to get.... here prices
               )
```


## Some plots for exploration...

```{r}
data_stocks %>% glimpse()
```

```{r}
data_stocks %>% head()
```

```{r}
data_stocks %>%
  ggplot(aes(x = date, y = adjusted,)) +
  geom_line() +
  labs(x = 'Date', y = "Adjusted Price") 
```

# Preprocessing

## Limit data

```{r}
data <- data_stocks %>%
  rename(index = date, value = adjusted) %>%
  select(index, value) %>%
  arrange(index) %>%
  drop_na()
```

## Remodel value as percentage change

* It is always easier to model change rather than absolute prices, so we create a variable measuring the percentage change of price instead

```{r}
data %<>%
  mutate(value = (value - lag(value,1)) / lag(value,1) ) %>%
  drop_na()
```

```{r}
data %>%
  ggplot(aes(x = index, y = value)) +
  geom_line() +
  labs(x = 'Date', y = "Price change in pct") 
```



## Train & Test split

* We do a time-series split which keeps the sequencing of the data

```{r}
# We use time_splits here to maintain the sequences
data_split <- data %>% initial_time_split(prop = 0.75)
```

```{r}
data_train <- data_split %>% training()
data_test <- data_split %>% testing()
```

* Lets see from where till when the train/test samples are

```{r}
# See ehat we got
data_train %>% pull(index) %>% min()
data_train %>% pull(index) %>% max()
data_test %>% pull(index) %>% min()
data_test %>% pull(index) %>% max()
```

## Define a reciepe

* We only apply min-max scaling herewith `step_range`

```{r}
data_recipe <- data_train %>%
  recipe(value ~ .) %>% 
  step_range(value, min = -1, max = 1)  %>%
  step_arrange(index) %>%
  prep()
```

* We save the min and max to rescale later again

```{r}
# Preserve the values for later (to reconstruct original values)
prep_history <- tibble(
  min = data_recipe$steps[[1]]$ranges[1],
  max = data_recipe$steps[[1]]$ranges[2]
)
```

```{r}
prep_history
```

## Get processedv train & test data

```{r}
# Number of lags
n_lag = 1
```

* We now create a x and y split. Since we here always predict the next observation, that's easy. We will just set y= lead(x, 1)
* We treplace the last missing observation with the lagged value

```{r}
# TRain data
x_train <- data_recipe %>% juice() 

y_train <- data_recipe %>%  juice() %>%
  mutate(value = value %>% lead(1)) %>%
  mutate(value = ifelse(is.na(value), lag(value, 1), value))

# And the same for the test data
x_test <- data_recipe %>% bake(data_test) 

y_test <- data_recipe %>%  bake(data_test) %>%  
  mutate(value = value %>% lead(1)) %>%
  mutate(value = ifelse(is.na(value), lag(value, 1), value))
```

## Transform to a 3d tensor for keras

```{r}
# TRansforming the x sequence to a 3d tensor (necessary for LSTMs)
x_train_arr <- x_train %>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))
x_test_arr <- x_test %>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))

y_train_arr <- y_train %>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1))
y_test_arr <- y_test %>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1))
```

# Setting up the LSTM

# LSTM

## Define model


```{r}
model <- keras_model_sequential() %>%
  # # First LSTM Layer
  layer_lstm(units            = 64, 
             input_shape= c(1, 1), #  timesteps, features
             return_sequences = TRUE, 
             ) %>% 
  # Second LSTM Layer
  layer_lstm(units            = 64, 
             dropout	        = 0.1,
             recurrent_dropout= 0.1,
             #stateful         = TRUE,
             return_sequences = FALSE) %>% 
  # A dropout layer
  layer_dropout(rate = 0.1) %>%
  # A DENSE LAYER IN BETWEEN
  layer_dense(units = 32, activation = 'relu') %>%
  #Final prediction layer
  layer_dense(units = 1, activation = 'linear')


# Compile model
model %>% 
  compile(loss = "mse", 
          metric = 'mse', 
          optimizer = "adam")
```

```{r}
model %>% summary()
```

## Fitting the model

* Next, we can fit our LSTM using a for loop (we do this to manually reset states). 
* We set `shuffle = FALSE` to preserve sequences

```{r}
hist_model <- model %>% fit(x          = x_train_arr, 
                            y          = y_train_arr, 
                            epochs     = 10,
                            verbose    = 1, 
                            batch_size = 64,
                            validation_split = 0.25, 
                            shuffle    = FALSE)
```

```{r}
hist_model %>% plot()
```


```{r}
model %>% evaluate(x_test_arr, y_test_arr)
```

## Predicting Stock changes

* We first predict the output of our test data

```{r}
model_pred <- model %>% predict(x_test_arr)
```

* However, we need to rescale the output. For min-max scaling, this function will do the trick

```{r}
reverse_minmax <- function(x, min, max) {
  x_re <- (x*(max - min)) + min
  }
```

* We apply it with our data and the saved min and max values from the recipe

```{r}
model_pred_rescaled <- model_pred %>% reverse_minmax(min = prep_history$min, max = prep_history$max)
```

Plot the results:

```{r}
eval <- tibble(
  index = data_test %>% pull(index),
  truth = data_test %>% pull(value),
  pred = model_pred_rescaled
) 
```

```{r}
eval %>% 
  pivot_longer(-index) %>%
  ggplot(aes(x = index, y = value, col = name)) +
  geom_line()
```






<!----
# Multi-episode LSTM

## Transform to a 3d tensor for keras

```{r}
tsteps_x = 5
tsteps_y = 5
```


```{r}
train_arr <- x_train %>% pull(value) %>% as.numeric() %>% matrix(ncol = (tsteps_x + tsteps_y))
```

```{r}
x_train_arr <- train_arr[,1:tsteps_x] %>% array_reshape(dim = c(length(.), 1, 1))
```


```{r}
#x_train %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))
#x_test %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))

#y_train %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1))
#y_test %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1))
```

---->