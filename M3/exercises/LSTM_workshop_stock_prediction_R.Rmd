---
title:  'Sequence-2-Sequence forecasting (R)'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```


```{r}
library(tidyverse)
library(magrittr)
library(keras)
```

```{r}
# others
library(tidyquant) # My favorite package to get stock data
library(tidymodels) # Or only load the 'rsample' and recipes on its own

# Time Series
# library(feasts) # We don t really need it
```

# Workshop Stock prediction

Task:

1. Get some stock data (tip: Use tidyquant)
    * Limit yourself for now to on e stock
    * Limit yourself to one variable (preferably some price data)
2. Develop a one-step ahead prediction of prices (or their movements)

# Load some data

## Select a stock abnd load the data

```{r}
tickers = c("AAPL", "NFLX", "AMZN") # Just for fun, we get a couple of stocks
            
data_stocks <- tq_get(tickers,
               from = "2005-01-01",
               to = "2020-11-30",
               get = "stock.prices" # What we want to get.... here prices
               )
```


## Some plots for exploration...


```{r}
data_stocks %>% glimpse()
```

```{r}
data_stocks %>% head()
```

```{r}
data_stocks %>%
  filter(symbol == 'AAPL') %>%
  ggplot(aes(x = date, y = adjusted,)) +
  geom_line() +
  labs(x = 'Date', y = "Adjusted Price") 
```

```{r}
data_stocks %>%
  ggplot(aes(x = date, y = adjusted, col = symbol)) +
  geom_line() +
  labs(x = 'Date', y = "Adjusted Price") 
```

# Preprocessing

## Limit data

```{r}
data <- data_stocks %>%
  filter(symbol == 'AAPL') %>%
  rename(index = date, value = adjusted) %>%
  select(index, value) %>%
  arrange(index) %>%
  drop_na()
```

## Train & Test split

```{r}
# We use time_splits here to maintain the sequences
data_split <- data %>% initial_time_split(prop = 0.75)
```

```{r}
data_train <- data_split %>% training()
data_test <- data_split %>% testing()
```

```{r}
# See ehat we got
data_train %>% pull(index) %>% min()
data_train %>% pull(index) %>% max()
data_test %>% pull(index) %>% min()
data_test %>% pull(index) %>% max()
```

## Define a reciepe

We only apply min-max scaling hee

```{r}
data_recipe <- data_train %>%
  recipe(value ~ .) %>% 
  step_range(value, min = -1, max = 1)  %>%
  step_arrange(index) %>%
  prep()
```

```{r}
# Preserve the values for later (to reconstruct original values)
prep_history <- tibble(
  min = data_recipe$steps[[1]]$ranges[1],
  max = data_recipe$steps[[1]]$ranges[2]
)
```

```{r}
prep_history
```



* We now create a x and y split. Since we here always predict the next observation, that's easy. We will just set y= lead(x, 1)
* We therefore also delete the last observation in train and the first in test

## Get processedv train & test data

```{r}
# Number of lags
n_lag = 1
```


```{r}
# TRain data
x_train <- data_recipe %>% 
  juice() %>%  
  slice(1:(n()-n_lag))

y_train <- data_recipe %>%  
  juice() %>%
  mutate(value = value %>% lead(1)) %>%
  slice((n_lag+1):n())

# And the same for the test data
x_test <- data_recipe %>% 
  bake(data_test) %>%  
  slice(1:(n()-n_lag))

y_test <- data_recipe %>%  
  bake(data_test) %>%  
  mutate(value = value %>% lead(1)) %>%
  slice((n_lag+1):n())
```

## Transform to a 3d tensor for keras

```{r}
x_train %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))
x_test %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))

y_train %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))
y_test %<>% pull(value) %>% as.numeric() %>% array_reshape(dim = c(length(.), 1, 1))
```

# Setting up the LSTM


# LSTM

## Define model

```{r}
tsteps       <- 1
batch_size   <- 1 # (nrow(x_train) / 10) %>% as.integer()
train_length <- nrow(x_train)
epochs       <- 10
```


```{r}
model <- keras_model_sequential() %>%
  # # First LSTM Layer
  layer_lstm(units            = 32, 
             batch_input_shape= c(batch_size, tsteps, 1), # timesteps, features
             return_sequences = FALSE, 
             stateful         = TRUE) %>% 
  layer_dense(units = 32, activation = 'relu') %>%
  # Second LSTM Layer
  #layer_lstm(units            = 64, 
  #           return_sequences = FALSE, 
  #           dropout	        = 0.1,
  #           recurrent_dropout= 0.1,
  #           stateful         = TRUE) %>% 
  # Final prediction layer
  layer_dense(units = 1, activation = 'linear')


# Compile model
model %>% 
  compile(loss = "mse", 
          metric = 'mse', 
          optimizer = "adam")
```

```{r}
model %>% summary()
```

## Fitting the model

* Next, we can fit our stateful LSTM using a for loop (we do this to manually reset states). 
* We set `shuffle = FALSE` to preserve sequences
* We manually reset the states after each epoch using `reset_states()`. Therefore we have to train the single epochs in loops. Otherwise, the states would be carried over between the epocs.


```{r}
for (i in 1:epochs) {
  model %>% fit(x          = x_train, 
                y          = y_train, 
                epochs     = 1, 
                verbose    = 1, 
                shuffle    = FALSE)
  
  model %>% reset_states()
  cat("Epoch: ", i)
}
```

```{r}
model %>% evaluate(x_test, y_test)
```

```{r}
model_pred <- model %>% predict(y_test)
```


