---
title: 'NLP workshop - Exploring Presidential Debate on twitter'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```

```{r}
### Load standardpackages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)
```

```{r}
library(tidytext)
```


# Download the data

```{r}
# download and open some Trump tweets from trump_tweet_data_archive
library(jsonlite)
tmp <- tempfile()
download.file("https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pol_tweets.gz", tmp)

tweets_raw <- stream_in(gzfile(tmp, "pol_tweets"))
```

```{r}
tweets_raw %>% glimpse()
```

```{r}
tweets <- tibble(ID = colnames(tweets_raw[[1]]), 
                 text = tweets_raw[[1]] %>% as.character(), 
                 labels = tweets_raw[[2]] %>% as.logical())
rm(tweets_raw)
```

```{r}
tweets %>% glimpse()
```

```{r}
tweets %<>%
  filter(!(text %>% str_detect('^RT')))
```

```{r}
tweets %>% head()
```


# Tidying

```{r}
tweets_tidy <- tweets %>%
  unnest_tokens(word, text, token = "tweets") 
```

```{r}
tweets_tidy %>% head(50)
```

```{r}
tweets_tidy %>% count(word, sort = TRUE)
```



# Preprocessing

```{r}
# preprocessing
tweets_tidy %<>%
  filter(!(word %>% str_detect('@'))) %>% # remove hashtags and mentions
#  mutate(word = word %>% str_remove_all('[^[:alnum:]]')) %>% ## remove all special characters
  filter(str_length(word) > 2 ) %>% # Remove words with less than  3 characters
  group_by(word) %>%
  filter(n() > 100) %>% # remove words occuring less than 100 times
  ungroup() %>%
  anti_join(stop_words, by = 'word') # remove stopwords
```

# TFIDF

```{r}
# top words
tweets_tidy %>%
  count(word, sort = TRUE) %>%
  head(20)
```

```{r}
# TFIDF weights
tweets_tidy %<>%
  add_count(ID, word) %>%
  bind_tf_idf(term = word,
              document = ID,
              n = n)
```

```{r}
tweets_tidy %>%
  head()
```

```{r}
# TFIDF topwords
tweets_tidy %>%
  count(word, wt = tf_idf, sort = TRUE) %>%
  head(20)
```

# Inspecting

```{r}
labels_words <- tweets_tidy %>%
  group_by(labels) %>%
  count(word, wt = tf_idf, sort = TRUE, name = "tf_idf") %>%
  slice(1:20) %>%
  ungroup() %>%
  mutate(word = reorder_within(word, by = tf_idf, within = labels)) 
```

```{r}
labels_words %>%
  ggplot(aes(x = word, y = tf_idf, fill = labels)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~labels, ncol = 2, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```


# Predictive model

```{r}
library(tidymodels)
```


```{r}
data <- tweets %>%
  select(labels, text) %>%
  rename(y = labels) %>%
  mutate(y = y  %>% as.factor()) 
```


## Training & Test split

```{r}
data_split <- initial_split(data, prop = 0.75, strata = y)

data_train <- data_split  %>%  training()
data_test <- data_split %>% testing()
```

## Preprocessing pipeline

```{r}
library(textrecipes)
```

```{r}
data_recipe <- data_train %>%
  recipe(y ~.) %>%
  themis::step_downsample(y) %>%
  step_filter(!(text %>% str_detect('^RT'))) %>%
  step_filter(text != "") %>%
  step_tokenize(text, token = "tweets") %>%
  step_tokenfilter(text, min_times = 100) %>%  
  step_stopwords(text, keep = FALSE) %>%
  step_tfidf(text) %>%
  prep() # NOTE: Only prep the recipe when not using in a workflow
```

```{r}
data_recipe
```


```{r}
data_recipe_reduced <- data_train %>%
  recipe(y ~.) %>%
  themis::step_downsample(y) %>%
  step_filter(!(text %>% str_detect('^RT'))) %>%
  step_filter(text != "") %>%
  step_tokenize(text, token = "tweets") %>%
  step_tokenfilter(text, min_times = 100) %>%  
  step_stopwords(text, keep = FALSE) %>%
  step_tfidf(text) %>%
  step_pca(all_predictors()) %>%
  prep() # NOTE: Only prep the recipe when not using in a workflow
```

Since we will not do hyperparameter tuning, we directly bake/juice the recipe

```{r}
data_train_prep <- data_recipe %>% juice()
data_test_prep <- data_recipe %>% bake(data_test)
```


## Defining the models

```{r, warning=FALSE}
all_cores <- parallel::detectCores(logical = FALSE)

library(doParallel)
cl <- makePSOCKcluster(all_cores)
registerDoParallel(cl)
```


```{r}
model_en <- logistic_reg(mode = 'classification', 
                         mixture = 0.5, 
                         penalty = 0.5) %>%
  set_engine('glm', family = binomial) 
```

```{r}
model_xg <- boost_tree(mode = 'classification') %>%
  set_engine('ranger') 
```


## Define the workflow

We will skip the workflow step this time, since we do not evaluate different models against each others.

## fit the model

```{r}
fit_en <- model_en %>% fit(formula = y ~., data = data_train_prep)
```


```{r}
pred_collected <- tibble(
  truth = data_train_prep %>% pull(y),
  pred = fit_en %>% predict(new_data = data_train_prep) %>% pull(.pred_class),
  pred_prob = fit_en %>% predict(new_data = data_train_prep, type = "prob") %>% pull(.pred_TRUE),
  ) 
```

```{r}
pred_collected %>% conf_mat(truth, pred)
```

```{r}
pred_collected %>% conf_mat(truth, pred) %>% summary()
```

Well... soso

# Using the model for new prediction

## Simple test

```{r}
pred_own = tibble(text = 'USA USA WE NEED A WALL TO KEEP THE MEXICANS AND ALL SHITHOLE COUNTRIES OUT!')
```


```{r}
fit_en %>% predict(new_data = data_recipe %>% bake(pred_own))
```


## New data


```{r}
# download and open some Trump tweets from trump_tweet_data_archive
library(jsonlite)
tmp <- tempfile()
download.file("https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/pres_debate_2020.gz", tmp)

tweets_raw <- stream_in(gzfile(tmp, "pres_debate_2020"))
```

```{r}
tweets_raw %>% glimpse()
```


```{r}
tweets_new <- tibble(ID = tweets_raw[[1]]$id, 
                     text = tweets_raw[[1]]$tweet)
rm(tweets_raw)
```

```{r}
pred_new <- fit_en %>% predict(new_data = data_recipe %>% bake(tweets_new))
```

To be continued by you :)


# Topic models

```{r}
# for LDA analysis
library(topicmodels)
```

### Preparing the Data

```{r}
tweets_dtm <- tweets_tidy %>%
  cast_dtm(document = ID, term = word, value = n)
```

Lets take a look:

```{r}
tweets_dtm
```

We see again hat the matrix is still rather sparse, which is an artefact of text data generally, but even more so when using twitter data. Lets try to see if we could reduce that somewhat by deleting less often used terms.

```{r}
library(tm)
tweets_dtm %>% removeSparseTerms(sparse = .99)
```

```{r}
tweets_dtm %>% removeSparseTerms(sparse = .999)
```

```{r}
tweets_dtm %>% removeSparseTerms(sparse = .9999)
```

Ok, we might have to accept a high level of sparsity in order to still have a meaningful number of unique words.

Now we can perform a LDA, using the more accurate Gibbs sampling as `method`.

```{r}
tweets_lda <- tweets_dtm %>% 
  LDA(k = 6, method = "Gibbs",
      control = list(seed = 1337))
```

### $\beta$: Word-Topic Association

$\beta$ is an output of the LDA model, indicating the propability that a word occurs in a certain topic. Therefore, loking at the top probability words of a topic often gives us a good intuition regarding its properties.

```{r}
# LDA output is defined for tidy(), so we can easily extract it
lda_beta <- tweets_lda %>% 
  tidy(matrix = "beta") %>%
  group_by(topic) %>%
  arrange(topic, desc(beta)) %>%
  slice(1:10) %>%
  ungroup() 
```

```{r}
lda_beta %>% head()
```



```{r}
# Notice the "reorder_within()"
lda_beta %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 2, scales = "free")
```

### $\gamma$: Document-Topic Association

In LDA, documents are represented as a mix of topics. This association of a document to a topic is captured by $\gamma$

```{r}
lda_gamma <- tweets_lda %>% 
  tidy(matrix = "gamma")
```

```{r}
lda_gamma %>% head()
```

```{r}
lda_gamma %>%
  ggplot(aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```

```{r}
lda_gamma %>% 
  group_by(topic) %>%
  arrange(desc(gamma)) %>%
  slice(1:3) %>%
  ungroup() %>%
  left_join(tweets %>% select(ID, text), by = c('document' = 'ID'))
```


```{r}
lda_gamma %>%
  ggplot(aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 2) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```

```{r}
top_topics <- tweets_lda %>% 
  tidy(matrix = "gamma")  %>%
  group_by(document) %>%
  top_n(1, wt = gamma) %>%
  ungroup()
```

```{r}
top_topics %>%
  count(topic)
```

```{r}
x <- lda_gamma %>%
  left_join(tweets %>% select(ID, labels), by = c('document' = 'ID'))
```

```{r}
x %>%
  group_by(labels, topic) %>%
  summarise(n = sum(gamma) ) %>%
  group_by(labels) %>%
  mutate(n = n / sum(n)) %>%
  ungroup()
```

# Endnotes

### Packages & Ecosystem

* [`tidytext`](https://github.com/juliasilge/tidytext)
* [`textrecipes`](https://textrecipes.tidymodels.org/)
* [`topicmodels`](https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf)

Further NLP packages ecosystem

* `tm` [here](https://cran.r-project.org/web/packages/tm/)
* `quanteda` [here](https://quanteda.io/), and many many great tutorials [here](https://tutorials.quanteda.io/)


### References 

* ulia Silge and David Robinson (2020). Text Mining with R: A Tidy Approach, Oâ€™Reilly. Online available [here](https://www.tidytextmining.com/)
   * [Chapter 6](https://www.tidytextmining.com/topicmodeling.html): Introduction topic models

### Further sources

* [Julia Silge's Blog](https://juliasilge.com/): Full of great examples of predictive modeling, NLP, and the combination fo both, using tidy ecosystems

### Session Info

```{r}
sessionInfo()
```
