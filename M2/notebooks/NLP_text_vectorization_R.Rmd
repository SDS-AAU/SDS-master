---
title: '(Somewhat) advanced NLP: text vectorization'
author: "Daniel S. Hain (dsh@business.aau.dk)"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_notebook:
    code_folding: show
    df_print: paged
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
    theme: flatly
---

```{r setup, include=FALSE}
### Generic preamble
rm(list=ls())
Sys.setenv(LANG = "en") # For english language
options(scipen = 5) # To deactivate annoying scientific number notation

### Knitr options
library(knitr) # For display of the markdown
knitr::opts_chunk$set(warning=FALSE,
                     message=FALSE,
                     comment=FALSE, 
                     fig.align="center"
                     )
```

```{r}
### Load standardpackages
library(tidyverse) # Collection of all the good stuff like dplyr, ggplot2 ect.
library(magrittr) # For extra-piping operators (eg. %<>%)
```

```{r}
library(tidytext)
```

# This session


# Refresher:

![](https://sds-aau.github.io/SDS-master/00_media/nlp_tidyworkflow.png)


# Bag of words model

* In order for a computer to understand text we need to somehow find a useful representation.
* If you need to compare different texts e.g. articles, you will probably go for keywords. These keywords may come from a keyword-list with for example 200 different keywords
* In that case you could represent each document with a (sparse) vector with 1 for "keyword present" and 0 for "keyword absent"
* We can also get a bit more sophoistocated and count the number of times a word from our dictionary occurs.
* For a corpus of documents that would give us a document-term matrix.

![example](https://i.stack.imgur.com/C1UMs.png)

Let's try creating a bag of words model from our initial example.

```{r}
text <- tibble(id = c(1:6),
               text = c('A text about cats.',
                        'A text about dogs.',
                        'And another text about a dog.',
                        'Why always writing about cats and dogs, always dogs?',
                        'There are too little text about cats but to many about dogs',
                        'Cats, cats, cats! I love cats soo much. Cats are way better than dogs'))
```

```{r}
text_tidy <- text %>% 
  unnest_tokens(word, text, token = 'words') %>% 
  count(id, word)
```


## The document-term matrix (DTM)

* The simplest form of vector representation of text is a ddocument-term matrix
* How to we get a document-term matrix now?
* We could do it by hand, with well-known `dplyr` syntax (Note: only works when you have one row per unique document-word pair)

```{r}
text_tidy %>%
  pivot_wider(names_from = word, values_from = n, values_fill = 0)
```

* We could also use `cast_dtm()` to create a DTM in the format of the `tm` package.

```{r}
text_dtm <- text_tidy %>%
  cast_dtm(id, word, n)
```

```{r}
text_dtm 
```

* We can simply convert ig to a tibble. Since there exists no direct transfer function, we have to first transform it to a matrix.
* Notice how we recover the rownames

```{r}
text_dtm %>% as.matrix() %>% as_tibble(rownames = 'id') 
```

* Sidenote: We can also tidy the DTM again to a tidy token-dataframe.

```{r}
text_dtm %>% tidy()
```
* We also can directly use a similar function to cast a sparse matrix (which we for sure then also could transform to a tibble again)

```{r}
text_tidy %>% cast_sparse(row = id, column = word, value = n)
```

* Finally, we could just apply a text recipe here

```{r}
library(recipes)
library(textrecipes)
```

```{r}
text %>%
  recipe(~.) %>% 
  step_tokenize(text, token = 'words') %>% # tokenize
  step_tf(text) %>% # TFIDF weighting
  prep() %>% juice()
```


## TF-IDF - Term Frequency - Inverse Document Frequency

* A token is important for a document if appears very often
* A token becomes less important for comparison across a corpus if it appears all over the place in the corpus
* *Cat* in a corpus of websites talking about cats is not that important

$$w_{i,j} = tf_{i,j}*log(\frac{N}{df_i})$$

- $w_{i,j}$ = the TF-IDF score for a term i in a document j
- $tf_{i,j}$ = number of occurence of term i in document j
- $N$ = number of documents in the corpus
- $df_i$ = number of documents with term i

```{r}
# TFIDF weights
text_tidy %<>%
  bind_tf_idf(term = word,
              document = id,
              n = n)
```

* We obviously could also cast a tf_idf weighted dtm...

```{r}
text_tidy %>%
  select(id, word, tf_idf) %>%
  pivot_wider(names_from = word, values_from = tf_idf, values_fill = 0)
```

* btw: this is equivalent to just running a textrecipe like that:

```{r}
text %>%
  recipe(~.) %>% 
  step_tokenize(text, token = 'words') %>% # tokenize
  step_tfidf(text) %>% # TFIDF weighting
  prep() %>% juice()
```

* A last reminder on the powerful `pairwise_xx()` functions from the `widyr` package
* For instance, pair

```{r}
library(widyr)
```

```{r}
text_tidy %>% pairwise_dist(id, word, tf_idf, method = "manhattan") %>%
  mutate(similarity = 1 - (distance / max(distance)) ) %>%
  select(-distance) %>%
  arrange(desc(similarity))
```



# Dimensionality reduction techniques

```{r}
rm(list=ls())
```

* Ok, lets get first some more interesting 

```{r}
text <- read_csv('https://github.com/SDS-AAU/SDS-master/raw/master/M2/data/cordis-h2020reports.gz')
```

```{r}
text %<>%
  rename(id = X1) %>%
  filter(language == 'en')
```



```{r}
# preprocessing
text_tidy %<>%
  #mutate(word = word %>% str_remove_all('[^[:alnum:]]')) %>% ## remove all special characters
  filter(str_length(word) > 2 ) %>% # Remove words with less than  3 characters
  group_by(word) %>%
  filter(n() > 100) %>% # remove words occuring less than 100 times
  ungroup() %>%
  anti_join(stop_words, by = 'word') # remove stopwords
```






```{r, include=FALSE}
text_dtm <- text %>%
  unnest_tokens(word, text, token = 'words') %>% 
  count(id, word) %>%
  pivot_wider(names_from = word, values_from = n, values_fill = 0)
```



## PCA

```{r}
text_pca <- text_dtm %>% 
  column_to_rownames('id') %>% 
  prcomp(center = TRUE, scale. = TRUE)
```

```{r}
text_pca %>% glimpse()
```

```{r}
text_pca[['x']]
```

* Again, alternatively with a recipe...

```{r}
text_pca <- text %>%
  recipe(~.) %>% 
  update_role(id, new_role = "id") %>%
  step_tokenize(text, token = 'words') %>% # tokenize
  step_tfidf(text, prefix = NULL) %>% # TFIDF weighting
  step_pca(all_predictors(), num_comp = 3) %>% # PCA
  prep() 
```

```{r}
text_pca %>% juice()
```

```{r}
text_pca %>%
  tidy(3) %>%
  filter(component %in% paste0("PC", 1:3)) %>%
  mutate(component = fct_inorder(component)) %>%
  ggplot(aes(value, terms, fill = terms)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~component, nrow = 1) +
  labs(y = NULL)
```


```{r}
library(embed)
```

```{r}
text_UMAP <- text %>%
  recipe(~.) %>% 
  update_role(id, new_role = "id") %>%
  step_tokenize(text, token = 'words') %>% # tokenize
  step_tfidf(text, prefix = NULL) %>% # TFIDF weighting
  step_umap(all_predictors(), n_neighbors = 2) %>%
  prep() 
```







## Topic Models: LDA




```{r}
#UMAP
```



# Embeddings (Bonus)

```{r}
library(textdata)

glove6b <- embedding_glove27b(dimensions = 100)
glove6b
# These mebeddings can now be loaded with step_wordembeddings
```




# Summary






