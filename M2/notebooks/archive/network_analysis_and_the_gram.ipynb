{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "network analysis and the gram",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SDS-AAU/M2-2019/blob/master/notebooks/network_analysis_and_the_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3AlnzwlSLcep"
      },
      "source": [
        "# Exploring Hashtag Networks on the Gram\n",
        "\n",
        "In this notebook, we will collect data from Instagram to construct (snowball) a network of hashtags as well as a bipartite network of Instagram users and hashtags.\n",
        "\n",
        "This work builds on [this post](https://medium.com/@marcosacj/creating-and-visualizing-a-complex-network-of-instagram-hashtags-based-on-posts-about-politics-2daf24f31088) but I've rewritten the code to a shorter more \"notebook-friendly\" format employing more dependencies (pandas, numpy & co). You can find the original code here.\n",
        "\n",
        "The networks in this example can be considered synthetic, since nodes and particularly edges represent virtual constructs rather than explicit connections.\n",
        "\n",
        "\n",
        "The notebook explores:\n",
        "\n",
        "- Graph construction (normal and bipartite)\n",
        "- Calculation of centrality indicators \n",
        "- Community detection\n",
        "- Projection of bipartite network\n",
        "\n",
        "Furthermore you will learn:\n",
        "\n",
        "- to make simple (public) API requests (API: Application Programming Interface) \n",
        "- parse json response\n",
        "- perform simple string manipulation/text-mining to extract features of interest (Transition into NLP)\n",
        "\n",
        "### So what?\n",
        "\n",
        "Such an analysis can be useful in marketing to identify sub-dicussions in a niche or related to a brand. We will detect popular hashtags within sub-niches that \"correlate\" with a topic of interest.\n",
        "Furthermore, we will identify accounts with high engagement (post-counts) within specific hashtag communities.\n",
        "\n",
        "Unfortunately Instagram, very recently (few days back), diesabled a simple public API that allowed to map usernames form user-ids. Therefore, we will use ```instaloader```, a module for interacting with Instagram. \n",
        "\n",
        "We will only use public data that does not require log-in. If you want to explore other graph structures on Instagram (e.g. follow-networks), have a look at Instabot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ThqBzEGoLTwH"
      },
      "source": [
        "### Importing Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xjYLaVyTZjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Installing instaloader\n",
        "!pip3 install instaloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tid_jgkpM1qh",
        "colab": {}
      },
      "source": [
        "# The requests library handles \"requests\" to APIs \n",
        "# similar to a browser that requests a webpage given a URL\n",
        "\n",
        "import requests as rq\n",
        "\n",
        "# A bit of a transition into NLP. The tweet tokenizer from the NLTK library will help us extract\n",
        "# the hashtags from post-text\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "# The networkx module for all network related manipulation + bipartite  + the community module for \n",
        "# community identification in large graphs (not part of networkx)\n",
        "import networkx as nx\n",
        "from networkx.algorithms import bipartite \n",
        "import community\n",
        "\n",
        "# Python's amazing iteration & combination library\n",
        "import itertools\n",
        "\n",
        "# The usual suspects\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Instaloader for mapping user-ids to usernames\n",
        "import instaloader\n",
        "L = instaloader.Instaloader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lEXYlnZvLTwb"
      },
      "source": [
        "### Global Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K3Gsb0qiMI53",
        "colab": {}
      },
      "source": [
        "# These things may change without a warning...\n",
        "\n",
        "# Instagram base url preffix\n",
        "tagurl_prefix = 'https://www.instagram.com/explore/tags/'\n",
        "\n",
        "# suffix to append to tag request url to retrieve data in JSON format\n",
        "tagurl_suffix = '/?__a=1'\n",
        "\n",
        "# suffix to end cursor when requesting posts by tag\n",
        "tagurl_endcursor = '&max_id='\n",
        "\n",
        "# a generic media post preffix (concat with media shortcode to view)\n",
        "posturl_prefix = 'https://www.instagram.com/p/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-MAqIYZLWaY",
        "colab": {}
      },
      "source": [
        "# target initial tags (we will run this with only one tag but the code can take multiple - just extend the list)\n",
        "\n",
        "tags = ['fitnessworlddk']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tYszKu8aLTxa",
        "colab": {}
      },
      "source": [
        "# urls to initial tags using the above url-components\n",
        "queries = [ tagurl_prefix + tag + tagurl_suffix for tag in tags ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ3O6SlpTZjj",
        "colab_type": "text"
      },
      "source": [
        "### Getting the Data\n",
        "\n",
        "The response structure of this Insta endpoint is not really straightforward. You can read more about it in the original post.\n",
        "The data is most likely composed on request by some large-scale graph database at returned. Instagram obviously assumes that the receiving site is a browser exploring public posts.\n",
        "\n",
        "We also don't get all posts for some hashtag right away but a \"page\" ~25 posts.\n",
        "\n",
        "To receive further posts, we need to pass a new requests specifying \"our position\" by providing an end_cursor.\n",
        "\n",
        "This **end cursor** can be found in\n",
        "\n",
        "```response['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']````\n",
        "\n",
        "\n",
        "\n",
        "#### Some thoughts on JSON\n",
        "\n",
        "This brings us to JSON. Think of JSON objects as of combinations of dictionaries and lists that can contain most Python objects (e.g. lists, dictionaries, tuples, strings, ints etc.) that can be represented as text. Once parsed you can operate JSON objects just as any other dictionary or list in Python.\n",
        "More about JSON - here:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0iSlXeSTZjk",
        "colab_type": "text"
      },
      "source": [
        "#### Where are the individual posts?\n",
        "\n",
        "They hide in ```response['graphql']['hashtag']['edge_hashtag_to_media']['edges']``` here you will find a list of dictionaries. If you think this is convoluted, wait until you see the structure of each post.\n",
        "\n",
        "#### How do we collect them?\n",
        "\n",
        "We create an empy list and iterate trough hashtags and iterations (deepth) and finally extend the empty list with the elements in the recent request.\n",
        "Try to run ```edges[0]```once you collected everything to see the structure of one of the posts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ooHPeCGC9V3f",
        "colab": {}
      },
      "source": [
        "edges = []\n",
        "for q in queries:    \n",
        "    for i in range(10): # how many iterations/deepth ?\n",
        "      r = rq.get(q).json()\n",
        "      end_cursor = r['graphql']['hashtag']['edge_hashtag_to_media']['page_info']['end_cursor']\n",
        "      edges.extend(r['graphql']['hashtag']['edge_hashtag_to_media']['edges'])\n",
        "      print(i)\n",
        "      q = q + tagurl_endcursor + end_cursor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O8azO1K-pQNO",
        "colab": {}
      },
      "source": [
        "edges[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXQDaLOyTZjq",
        "colab_type": "text"
      },
      "source": [
        "### Bringing the collected data into useful format...\n",
        "\n",
        "In the next step we will take the rich raw posts data and extract only the information that we need for our analysis. We will just cut out owner-id (account that posted), a shortcode that we can use to identify the post and get more data on it in future if needed, and the text including the hashtags.\n",
        "\n",
        "To make things more compact we not only extract the raw data but we also preprocess a bit.\n",
        "\n",
        "The hashtags are incorporated within the post-text. Therefore, we pass the text of each post through a tokenizer, that identifies individual words and elements (such as emoji). We use the tweet-tokenizer from the NLTK library, which is made for working with social media data.\n",
        "\n",
        "```\n",
        "  tokens = tknzr.tokenize(text)\n",
        "  tags = [x.strip('#') for x in tokens if x.startswith('#')]\n",
        "```\n",
        "\n",
        "The first line turns the text of the post in a list of tokens (words & co.). The second line picks out only the elements that start with a \"#\" and strips the \"#\" when adding them to a list.\n",
        "\n",
        "Then we construct a dictionary with these values and append it to a list.\n",
        "\n",
        "This gives us a list of dicitonaries - something that we can pass to Pandas to get a dataframe we can work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7Kx30gQfGesT",
        "colab": {}
      },
      "source": [
        "post_dicts = [] #empty list\n",
        "\n",
        "for post in edges: #iterate all raw posts\n",
        "\n",
        "  if post['node']['edge_media_to_caption']['edges'] == []: # hop to the next if no text in the post\n",
        "    continue\n",
        "    \n",
        "  post_dict = {} # empty dictionary\n",
        "  id_owner = post['node']['owner']['id'] # pick out user-id\n",
        "  shortcode = post['node']['shortcode'] # pick out short post identifier\n",
        "  text = post['node']['edge_media_to_caption']['edges'][0]['node']['text'] # pick out post text\n",
        "  \n",
        "  # Pick hashtags from text\n",
        "  tokens = tknzr.tokenize(text)\n",
        "  tags = [x.strip('#') for x in tokens if x.startswith('#')]\n",
        "\n",
        "  # fill in dictionary with values\n",
        "  post_dict['id_owner'] = id_owner\n",
        "  post_dict['shortcode'] = shortcode\n",
        "  post_dict['tags'] = tags\n",
        "  post_dict['text'] = text\n",
        "\n",
        "  post_dicts.append(post_dict) #append the dictionary to a list of post-dictionaries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Jj_052CyiEsJ",
        "colab": {}
      },
      "source": [
        "# Create DF\n",
        "posts_df = pd.DataFrame(post_dicts)\n",
        "\n",
        "# Remove hashtags that are not a hashtag (emptyspace & mistakes)\n",
        "posts_df['tags'] = posts_df['tags'].map(lambda t: [x for x in t if x.isalnum()])\n",
        "\n",
        "# Kick out posts with 0 hashtags\n",
        "posts_df = posts_df[posts_df['tags'].map(len) != 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bAhRSZDTZjv",
        "colab_type": "text"
      },
      "source": [
        "#### Simple stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kzMn2oro_cY",
        "colab": {}
      },
      "source": [
        "# People with most posts\n",
        "\n",
        "posts_df['id_owner'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rYb2sAne0l-J",
        "colab": {}
      },
      "source": [
        "# Look up who these people are (this line gets us also other information about the user)\n",
        "profile = instaloader.Profile.from_id(L.context, 6047745288)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hm1jzRj2uwG5",
        "colab": {}
      },
      "source": [
        "profile.username"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2jHzc1GciSHV"
      },
      "source": [
        "### Creating a graph\n",
        "\n",
        "Networkx is rather \"base-Pythonic\" in it's syntax. But not too crazy after all. The documentation is nice and clear...and you probably have seen the most in the DC courses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ou3by8bBjMey",
        "colab": {}
      },
      "source": [
        "# Create empty undirected Graph\n",
        "G = nx.Graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Ni-OPMTZj-",
        "colab_type": "text"
      },
      "source": [
        "We will construct the graph from hashtag combinations of each post. We will use ```itertools.combinations``` for that. Given a list of n objects this will create all possible unique combinations of size k (which we set to 2). Note, that we can build up the Graph sequentially. An edgelist contains all data we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O_R6hQ5siUZK",
        "colab": {}
      },
      "source": [
        "# Create the graph\n",
        "for i in posts_df['tags']:\n",
        "  G.add_edges_from(list(itertools.combinations(i,2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeJ0DcbyTZkA",
        "colab_type": "text"
      },
      "source": [
        "#### Filtering the Graph\n",
        "\n",
        "It can be a good idea to filter the Graph before analysing. For instance, we can remove all hashtags with low degree-centrality. This can be interpreted as - kicking out made up hashtags or extremely underused ones. We will calculate a percentile threshold and exclude everything under it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hK_OpLvLji2q",
        "colab": {}
      },
      "source": [
        "# Calculating degree centrality for the Graph\n",
        "degree_centrality = nx.degree_centrality(G)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fA8M5xdawt2Z",
        "colab": {}
      },
      "source": [
        "# Getting a \"reasonable\" lower bound.\n",
        "perc_filter = np.percentile([v for u,v in degree_centrality.items()], 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sdUEyVq1xRdK",
        "colab": {}
      },
      "source": [
        "# Make a subgraph based on nodes with a degree_centrality over the threshold\n",
        "nodes_selected = [x for x,y in degree_centrality.items() if y >= perc_filter]\n",
        "\n",
        "G = G.subgraph(nodes_selected)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmyK4ricTZkN",
        "colab_type": "text"
      },
      "source": [
        "#### Analysing the Graph\n",
        "\n",
        "Now we are going to calculate some network indicators and once done, we will export a DataFrame analyse them further."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIVRMHEQx0Wy",
        "colab": {}
      },
      "source": [
        "# Recalculate degre-centrality and assign it as a node-attribute\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "nx.set_node_attributes(G, degree_centrality, 'degree')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MkoZIYy9txY3",
        "colab": {}
      },
      "source": [
        "# Same for Eigenvector Centrality\n",
        "eigenvector = nx.eigenvector_centrality(G)\n",
        "nx.set_node_attributes(G, eigenvector, 'eigenvector_centrality')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jSzoD3vfkglA",
        "colab": {}
      },
      "source": [
        "# Same for community detection\n",
        "communities = community.best_partition(G, resolution = 1)\n",
        "nx.set_node_attributes(G, communities, 'community')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8he4qJGuTZkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_df = pd.DataFrame(dict(G.nodes(data=True))).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rk21YTUnTZkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_df['community'].value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_pzqT80oLnp",
        "colab": {}
      },
      "source": [
        "# Find the 5 most popular hashtags for each identified community\n",
        "tag_per_com = graph_df.groupby('community')['degree'].nlargest(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W320m9YoTZkf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tag_per_com[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1Zftu2pTZkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's write the graph out to play around with it in Gephi\n",
        "nx.write_gexf(G, 'G_hashtags.gexf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LWgG_CYTZkj",
        "colab_type": "text"
      },
      "source": [
        "### Let's try out something else: a bipartite graph between users and hashtags\n",
        "\n",
        "Can we identify communities of users given their usage of hashtags?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XymDZyn_wHfs",
        "colab": {}
      },
      "source": [
        "# Create a new graph\n",
        "B = nx.Graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wTxPfMDVwWW_",
        "colab": {}
      },
      "source": [
        "# we will take the same data\n",
        "posts_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6H4M9FKGwJ3g",
        "colab": {}
      },
      "source": [
        "# We need to specify the nodes for level 0 - this will be our users\n",
        "B.add_nodes_from(list(set(posts_df.id_owner)), bipartite=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tM7w58yzwi4q",
        "colab": {}
      },
      "source": [
        "# Then we need to add hashtags nodes as level 1 nodes\n",
        "B.add_nodes_from(list(set(itertools.chain(*posts_df.tags))), bipartite=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KnnTj09Qw0tT",
        "colab": {}
      },
      "source": [
        "# This quick loop will generate edges between users and hashtags\n",
        "# Every time someone mentions a #hashtag, a link is created\n",
        "\n",
        "bi_edges = []\n",
        "for i in posts_df[['id_owner','tags']].iterrows(): # we do this row-by-row since each row is a post\n",
        "  id_owner = i[1]['id_owner']\n",
        "  for j in i[1]['tags']:\n",
        "    bi_edges.append((id_owner, j)) # edges are appended to a list as a tuple (id_owner, hashtag)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dbDNFeZH03mx",
        "colab": {}
      },
      "source": [
        "# Let's add the edges to our graph\n",
        "B.add_edges_from(bi_edges)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fNb4UWlTZl4",
        "colab_type": "text"
      },
      "source": [
        "In the next step we will project the graph onto the account-level. For this we need to get the nodesets of the 0 level. We also calculate the level 1 level (just because)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_6WNsuCt1OTG",
        "colab": {}
      },
      "source": [
        "# Extract a set of nodes with level 0\n",
        "top_nodes = {n for n, d in B.nodes(data=True) if d['bipartite']==0}\n",
        "\n",
        "# the remaining nodes are then level 1\n",
        "bottom_nodes = set(B) - top_nodes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xR79CPOi1XIM",
        "colab": {}
      },
      "source": [
        "# Let's project this graph using a weighted projection\n",
        "G_proj = bipartite.weighted_projected_graph(B, top_nodes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ac_yhg7n1qMY",
        "colab": {}
      },
      "source": [
        "# Again, we can identify communities\n",
        "bi_communities = community.best_partition(G_proj, resolution = 1)\n",
        "nx.set_node_attributes(G_proj, bi_communities, 'community')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gna3Vx043Aao",
        "colab": {}
      },
      "source": [
        "# Calculate eigenvector centrality and set it as an attribute\n",
        "bi_eigenvector = nx.eigenvector_centrality(G_proj)\n",
        "nx.set_node_attributes(G_proj, bi_eigenvector, 'eigenvector_centrality')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eR_xd_UuTZmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a new attribute \"activity\" - or propensity to spam\n",
        "nx.set_node_attributes(G_proj, dict(posts_df.id_owner.value_counts()), 'activity' )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aDiXEE4TZmZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Do spammers connect more in terms of spamming about the same stuff?\n",
        "\n",
        "print(nx.numeric_assortativity_coefficient(G_proj,'activity'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSAM-KBPTZmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_proj_df = pd.DataFrame(dict(G_proj.nodes(data=True))).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT1IpuS8TZmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "graph_proj_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym85hfMkTZmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the 5 most central for each identified community\n",
        "user_per_com = graph_proj_df.groupby('community')['eigenvector_centrality'].nlargest(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibw8vbsPTZm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "user_per_com"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "acSTPzx05IwN",
        "colab": {}
      },
      "source": [
        "profile = instaloader.Profile.from_id(L.context, 1929431148)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvSjd_UtTZm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(profile.biography)\n",
        "print(profile.username)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kXd0Q50g3PyV",
        "colab": {}
      },
      "source": [
        "nx.write_gexf(G_proj, 'G_proj.gexf')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}